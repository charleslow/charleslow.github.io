<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Yan 2025 - Semantic IDs - Chux&#x27;s Notebook</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../katex.min.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Chux&#x27;s Notebook</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="yan-2025---semantic-ids"><a class="header" href="#yan-2025---semantic-ids">Yan 2025 - Semantic IDs</a></h1>
<ul>
<li>Article: <a href="https://eugeneyan.com/writing/semantic-ids/">https://eugeneyan.com/writing/semantic-ids/</a></li>
<li>Code: <a href="https://github.com/eugeneyan/semantic-ids-llm">https://github.com/eugeneyan/semantic-ids-llm</a></li>
</ul>
<p>Eugene Yan's experience of training an LLM-recommender hybrid that can recommend items using natural language.</p>
<h2 id="extend-the-vocab"><a class="header" href="#extend-the-vocab">Extend the vocab</a></h2>
<p>Insert semantic ID tokens like <code>&lt;|sid_0|&gt;</code>, <code>&lt;|sid_1|&gt;</code> into the vocab. Sequences of these tokens will be used to represent the catalog. So an item may be represented like:</p>
<pre><code>&lt;|sid_start|&gt;&lt;|sid_173|&gt;&lt;|sid_324|&gt;&lt;|sid_764|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;
</code></pre>
<h2 id="data"><a class="header" href="#data">Data</a></h2>
<p>He used video games category of Amazon Reviews 2023, because it has rich product metadata and lots of behavioural data.</p>
<ul>
<li>Keep only products with titles longer that <code>20 chars</code> and description longer than <code>100 chars</code></li>
<li><code>66k</code> products</li>
<li><code>737k</code> rows of interactions</li>
</ul>
<h2 id="training-semantic-ids"><a class="header" href="#training-semantic-ids">Training Semantic IDs</a></h2>
<p>Used RQ-VAE to train semantic IDs. Will not expound on RQ-VAE here. Eugene used:</p>
<ul>
<li><code>3-level</code> codebook</li>
<li>Each level has <code>256</code> codes</li>
<li>This has collisions on around <code>10%</code> of the <code>66k products</code></li>
<li>Added a sequentially increasing token to each ID to ensure uniqueness</li>
</ul>
<h2 id="baselines"><a class="header" href="#baselines">Baselines</a></h2>
<ol>
<li>Train a SASRec model on semantic IDs to compare against traditional SASRec</li>
<li>Use Qwen3-Embedding-0.6B to encode product metadata into embeddings</li>
<li>Finetune Qwen3-8B to recommend items via semantic IDs</li>
</ol>
<h2 id="data-cleaning"><a class="header" href="#data-cleaning">Data Cleaning</a></h2>
<ul>
<li>Clean item descriptions using Gemini 2.5 Flash to remove HTML, reduce verbosity</li>
<li>Remove promotional text, standardize formatting for titles etc.</li>
<li>Augment data by extracting structured metadata like product type, platform, genre, hardware type etc.</li>
<li>Build user interaction histories</li>
</ul>
<h2 id="training-rq-vae"><a class="header" href="#training-rq-vae">Training RQ-VAE</a></h2>
<p>First, we get the <code>1,024</code> embeddings using Qwen3-0.6B, then l2-normalize. The RQ VAE consists of:</p>
<ul>
<li>An encoder</li>
<li>A codebook with 3 quantization levels</li>
<li>A symmetric decoder</li>
</ul>
<p>Some tricks:</p>
<ul>
<li>Use rotation trick as a replacement for the straight through estimator</li>
<li>Initialize codebooks with k means clustering</li>
<li>Reset unused codes</li>
<li>Large batch size</li>
</ul>
<p>Metrics for measuring quality of RQ-VAE:</p>
<ol>
<li><code>loss/reconstruction</code>: How well we can reconstruct the original item embeddings after compressing and decompressing</li>
<li><code>loss/vq</code>: Combined codebook and commitment loss across all levels.
<ul>
<li>Ensures that encoder outputs and codebook vectors stay close together</li>
</ul>
</li>
<li><code>loss/total</code>: sum of <code>loss/vq</code> and <code>loss/reconstruction</code></li>
<li><code>loss/validation</code>: total loss but on the held out validation set. Our <span style="color:orange">deciding metric</span>.</li>
<li><code>metrics/avg_residual_norm</code>: Leftover residuals between the quantized embedding and the original embedding</li>
<li><code>metrics/unique_ids_proportion</code>: % of items with unique IDS in a batch
<ul>
<li>Helps to check against codebook collapse</li>
<li>We want this metric to be high</li>
</ul>
</li>
<li>Codebook distribution
<ul>
<li>Plot the item distribution amongst the <code>256</code> codes at each level</li>
<li>It should look like a uniform distribution</li>
</ul>
</li>
</ol>
<p>Some hyperparameter tuning:</p>
<ul>
<li>Set <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.5</span></span></span></span>:
<ul>
<li>Tried <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">0.25</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0.5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1.0</span></span></span></span>. <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> is the commitment weight that balances reconstruction accuracy with codebook commitment</li>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.5</span></span></span></span> had the lowest validation loss, so it was chosen</li>
</ul>
</li>
<li>Investing in data cleaning significantly improved all the metrics</li>
</ul>
<h2 id="sasrec-comparison"><a class="header" href="#sasrec-comparison">SASRec comparison</a></h2>
<p>Eugene tested two SASRec variants:</p>
<ul>
<li>Traditional SASRec
<ul>
<li>Each item has a unique ID with no semantic meaning</li>
<li>Model uses 2 causal attention blocks, 64 dim hidden dimension, trained with binary cross entropy loss</li>
<li>Dot product of embeddings is used to generate scores</li>
</ul>
</li>
<li>Semantic SASRec
<ul>
<li>For semantic version, each item is a sequence of semantic tokens</li>
<li>Hence for each position, the model needs to generate a sequence of tokens to represent an item</li>
<li>Instead of binary cross entropy loss, we need to sum up the cross entropy loss at each position</li>
<li><span style="color:orange">Question</span>: does it make sense to add some weights to put more weightage on the first semantic token and decay it for subsequent positions?</li>
<li>Teacher forcing is used for this training</li>
<li>A larger model is used here, 4 causal attention blocks and 384 hidden dim</li>
</ul>
</li>
</ul>
<p>From this experiment, Eugene found that traditional SASRec is significantly better. But he puts this down to the difficulty of generating a sequence of tokens compared to directly generating one token. I also note that we are not using a pretrained LLM here, which means we are missing out on some pretrained capabilities that we could have tapped on.</p>
<h2 id="finetuning-qwen-8b"><a class="header" href="#finetuning-qwen-8b">Finetuning Qwen-8B</a></h2>
<p>Now finally we train the language model to converse in semantic IDs.</p>
<p>First, we build the training dataset of <code>4.2 million</code> conversations of various task types:</p>
<ul>
<li>Given a semantic ID, predict the item description</li>
<li>Given item description, predict the semantic ID</li>
<li>Predict the next item in the user's sequence</li>
<li>Understand relationships between item categories</li>
<li>Multi-hop reasoning</li>
</ul>
<p>Each of these are formatted as a conversation with a system prompt, user prompt and assistant response. Now, we finetune the model in two phases.</p>
<p>In <span style="color:orange">Phase 1</span>, we focus on extending the model's vocabulary.</p>
<ul>
<li>We add <code>1,027</code> new vocabulary items to the model's vocabulary, and resize the model's embedding matrix for them.</li>
<li>In this phase, we freeze all the model parameters except the input and output embedding layers, training <code>15%</code> of total parameters</li>
<li>Train for <code>1,000</code> steps with relatively high learning rate</li>
</ul>
<p>In <span style="color:orange">Phase 2</span>, we do the full training:</p>
<ul>
<li>Train for 3 epochs on the full dataset</li>
<li>3% warmup, lower learning rate, 8-bit AdamW optimizer for memory efficiency</li>
<li>Monitor progress with callbacks:
<ul>
<li>Track gradient norms</li>
<li>Measure hierarchical accuracy (I guess on different codebook granularities) on recommendation tasks</li>
<li>Sample model outputs on fixed set of conversational inputs</li>
</ul>
</li>
</ul>
<h2 id="inference"><a class="header" href="#inference">Inference</a></h2>
<p>Now the model is trained, we can have chat / recommendation conversations with it:</p>
<blockquote>
<p><span style="color:orange">User</span>: “Halo 3 Limited Edition -Xbox 360”, “Lost Planet: Extreme Condition - Xbox 360”, “Lost Planet: Extreme Condition Collector’s Edition -Xbox 360” &lt;|rec|&gt;</p>
<p><span style="color:orange">Model</span>: <code>&lt;|sid_start|&gt;&lt;|sid_64|&gt;&lt;|sid_351|&gt;&lt;|sid_733|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;</code></p>
</blockquote>
<p>The model can also explain its recs:</p>
<blockquote>
<p><span style="color:orange">User</span>: I just finished “Dragon Quest Heroes II Explorer’s Edition - PlayStation 4”. Suggest another &lt;|rec|&gt; and explain why:</p>
<p><span style="color:orange">Model</span>: <code>&lt;|sid_start|&gt;&lt;|sid_139|&gt;&lt;|sid_289|&gt;&lt;|sid_534|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;</code></p>
<p>If you liked Dragon Quest Heroes II, you might like Nights of Azure because both are action RPGs for the PlayStation 4 with a focus on combat and character progression. Both games offer a narrative-driven experience with a strong emphasis on combat mechanics, suggesting a shared player interest in this genre.</p>
</blockquote>
<p>We can even have multi-turn conversation with the model:</p>
<blockquote>
<p><span style="color:orange">User</span>: I’m looking for games similar to Mario Kart. &lt;|rec|&gt;</p>
<p><span style="color:orange">Model</span>: <code>&lt;|sid_start|&gt;&lt;|sid_131|&gt;&lt;|sid_503|&gt;&lt;|sid_738|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;, &lt;|sid_start|&gt;&lt;|sid_131|&gt;&lt;|sid_260|&gt;&lt;|sid_577|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;, &lt;|sid_start|&gt;&lt;|sid_145|&gt;&lt;|sid_416|&gt;&lt;|sid_671|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;</code></p>
<p><span style="color:orange">User</span>: How about something similar but for Xbox? &lt;|rec|&gt;</p>
<p><span style="color:orange">Model</span>: <code>&lt;|sid_start|&gt;&lt;|sid_183|&gt;&lt;|sid_461|&gt;&lt;|sid_517|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;, &lt;|sid_start|&gt;&lt;|sid_183|&gt;&lt;|sid_461|&gt;&lt;|sid_562|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;,&lt;|sid_start|&gt;&lt;|sid_183|&gt;&lt;|sid_461|&gt;&lt;|sid_616|&gt;&lt;|sid_768|&gt;&lt;|sid_end|&gt;</code></p>
<p><span style="color:orange">User</span>: Suggest a name and description for the bundle.</p>
<p><span style="color:orange">Model</span>: Xbox Racing Legends: Speed &amp; Style Pack</p>
</blockquote>
<h2 id="code"><a class="header" href="#code">Code</a></h2>
<p>Here we deep dive into Eugene's code and how it is implemented. Most of the code is contained in the <a href="https://github.com/eugeneyan/semantic-ids-llm/tree/main/src"><code>/src</code> directory</a>.</p>
<ul>
<li><a href="#device_managerpy">device_manager.py</a></li>
<li><a href="#tokenize_itemspy">tokenize_items.py</a></li>
<li><a href="#embed_itemspy">embed_items.py</a></li>
<li><a href="#train_rqvaepy">train_rqvae.py</a>
<ul>
<li><a href="#quantizationoutput">QuantizationOutput</a></li>
<li><a href="#vectorquantizer">VectorQuantizer</a></li>
<li><a href="#rqvae">RQVAE</a></li>
</ul>
</li>
<li><a href="#finetune_qwen3_8b_vocabpy">finetune_qwen3_8b_vocab.py</a>
<ul>
<li><a href="#finetuneconfig">FineTuneConfig</a></li>
<li><a href="#extend_tokenizer">extend_tokenizer</a></li>
<li><a href="#prepare_model">prepare_model</a></li>
<li><a href="#load_sid_dataset">load_sid_dataset</a></li>
<li><a href="#datainspectioncallback">DataInspectionCallback</a></li>
<li><a href="#embeddingmonitorcallback">EmbeddingMonitorCallback</a></li>
<li><a href="#semanticidgenerationcallback">SemanticIDGenerationCallback</a></li>
<li><a href="#train_embeddings">train_embeddings</a></li>
</ul>
</li>
</ul>
<h3 id="device_managerpy"><a class="header" href="#device_managerpy">device_manager.py</a></h3>
<p>The <code>DeviceManager</code> detects the device (<code>cpu</code> or <code>cuda</code> or <code>mps</code>) and is instantiated early in the scripts. The interesting part <code>torch.set_float32_matmul_precision("high")</code> performed when device is <code>cuda</code>. It's supposed to speed up <code>float32</code> operations?</p>
<h3 id="tokenize_itemspy"><a class="header" href="#tokenize_itemspy">tokenize_items.py</a></h3>
<p>This script tokenizes the product descriptions of video games:</p>
<ul>
<li>Uses <code>Qwen/Qwen3-Embedding-0.6B</code></li>
<li>Batch size = <code>32</code>, max length = <code>2048</code></li>
<li>Reads the data from <code>data/output/Video_Games_items_updated.parquet</code> into polars</li>
<li>Looks for the <code>item_context</code> field (already preprocessed)</li>
<li>Uses the following prompt which will be tokenized</li>
</ul>
<pre><code>Instruct: Given a product description, generate a semantic embedding that captures
    its key features and characteristics.
Query: {original_item_text}
</code></pre>
<ul>
<li>Saves tokenized <code>input_ids</code> and <code>attention_masks</code> and saves them in <code>.npz</code> (compressed numpy) format using <code>np.savez_compressed</code></li>
</ul>
<h3 id="embed_itemspy"><a class="header" href="#embed_itemspy">embed_items.py</a></h3>
<p>Takes the tokenized file and embeds them.</p>
<ul>
<li>Writes embedded items into a parquet file</li>
</ul>
<h3 id="train_rqvaepy"><a class="header" href="#train_rqvaepy">train_rqvae.py</a></h3>
<p>Compresses an item embedding into hierarchical semantic IDs.</p>
<p>The <code>RQVAEConfig</code> defines the following parameters:</p>
<ul>
<li><code>item_embedding_dim</code>: embedding dim of our embedding model</li>
<li><code>encoder_hidden_dims</code>: <code>[512, 256, 128]</code> the size of the VAE encoder</li>
<li><code>codebook_embedding_dim</code>: <code>32</code> Dimension of codebook vectors
<ul>
<li><span style="color:orange">Qn:</span> this does not need to match qwen embedding dim?</li>
</ul>
</li>
<li><code>codebook_quantization_levels</code>: <code>3</code> levels in the codebook</li>
<li><code>codebook_size</code>: <code>256</code> number of codes per level</li>
<li><code>commitment_weight</code>: <code>0.25</code> Commitment loss weight (beta)</li>
<li><code>use_rotation_trick</code>: <code>True</code> Use rotation trick for better gradient flow</li>
<li><code>batch_size</code>: <code>32768</code> training batch size (why so large?)</li>
<li><code>gradient_accumulation_steps</code>: <code>1</code></li>
<li><code>num_epochs</code>: <code>20000</code></li>
<li><code>scheduler_type</code>: <code>cosine_with_warmup</code></li>
<li><code>warmup_start_lr</code>: <code>1e-8</code> used for cosine_with_warmup</li>
<li><code>warmup_steps</code>: <code>200</code> used for cosine_with_warmup</li>
<li><code>max_lr</code>: <code>3e-4</code> maximum learning rate (start of cosine)</li>
<li><code>min_lr</code>: <code>1e-6</code> minimum learning rate (end of cosine)</li>
<li><code>use_gradient_clipping</code>: <code>True</code></li>
<li><code>gradient_clip_norm</code>: <code>1.0</code></li>
<li><code>use_kmeans_init</code>: <code>True</code> initializes codebook vectors using k-means</li>
<li><code>reset_unused_codes</code>: <code>True</code> reset unused codes periodically to avoid collapse</li>
<li><code>steps_per_codebook_reset</code>: <code>2</code> Reset unused codebook codes every N steps</li>
<li><code>codebook_usage_threshold</code>: <code>1.0</code> only reset if usage falls below this proportion (0-1)</li>
<li><code>val_split</code>: <code>0.05</code></li>
<li><code>steps_per_train_log</code>: <code>10</code> log every N steps</li>
<li><code>steps_per_val_log</code>: <code>200</code> validate and checkpoint every N steps</li>
</ul>
<p><code>EmbeddingDataset</code> is a torch dataset holding the embeddings:</p>
<ul>
<li>Extracts all embeddings and holds them in a <code>torch.tensor</code> at init
<ul>
<li>Not worried about OOM?</li>
</ul>
</li>
</ul>
<h4 id="quantizationoutput"><a class="header" href="#quantizationoutput">QuantizationOutput</a></h4>
<p><code>QuantizationOutput</code> is used to hold data for one quantized item:</p>
<ul>
<li>Holds the local loss for one codebook layer</li>
<li>Subclasses <code>NamedTuple</code> which is more lightweight than <code>dataclass</code></li>
<li><code>quantized_st: Tensor</code>
<ul>
<li>The quantized vector which is passed onto the next layer</li>
<li>Has the "gradient trick" (either straight through or rotation trick) applied</li>
<li>Allows backpropagation into the encoder even though we passed through the non-differentiable codebook layer</li>
</ul>
</li>
<li><code>quantized: Tensor</code>
<ul>
<li>The raw nearest neighbour vectors from the codebook</li>
<li>No gradients</li>
<li><span style="color:orange">Observation</span> <code>quantized</code> and <code>quantized_st</code> should be identical in values, just that one has gradients attached</li>
</ul>
</li>
<li><code>indices: Tensor</code>
<ul>
<li>These are integer indices which represent the semantic IDs</li>
</ul>
</li>
<li><code>loss: Tensor</code>
<ul>
<li>The combined loss for this specific codebook layer</li>
<li><code>loss = codebook_loss + beta * commitment_loss</code></li>
</ul>
</li>
<li><code>codebook_loss: Tensor</code>
<ul>
<li>Measures how well the codebook vector matches the encoder output</li>
</ul>
</li>
<li><code>commitment_loss: Tensor</code>
<ul>
<li>Measures how well the encoder output matches the codebook vectors</li>
</ul>
</li>
</ul>
<h4 id="vectorquantizer"><a class="header" href="#vectorquantizer">VectorQuantizer</a></h4>
<p>The <code>VectorQuantizer</code> implements one layer of the codebook and is the meat of the logic for training RQVAE. It will be stacked together layer multiple times to form the codebook.</p>
<ul>
<li>At initialization:
<ul>
<li>Initializes with <code>RQVAEConfig</code> to hold parameters</li>
<li>Initialize <code>self.embedding</code> to an embedding of size <code>codebook_size=256, codebook_embedding_dim=32</code>
<ul>
<li>Uniform initialization <code>self.embedding.weight.data.uniform_(-1 / self.codebook_size, 1 / self.codebook_size)</code></li>
</ul>
</li>
<li>Registers some buffers for tracking codebook usage:
<ul>
<li><code>self.register_buffer("usage_count", torch.zeros(self.codebook_size))</code></li>
<li><code>self.register_buffer("update_count", torch.tensor(0))</code></li>
</ul>
</li>
</ul>
</li>
<li><code>find_nearest_codes(x)</code>:
<ul>
<li>Takes an input vector <code>x</code>, compares it to all vectors in the codebook, and returns the nearest one</li>
<li>Simply uses <code>torch.cdist</code> to compute distances, then <code>torch.argmin</code> to get the nearest</li>
<li>Returns a tuple of torch tensors:
<ul>
<li>The nearest index (i.e. codeword) to <code>x</code></li>
<li>The quantized embedding at the index position</li>
</ul>
</li>
</ul>
</li>
<li><code>forward(x)</code> -&gt; <code>QuantizationOutput</code>:
<ul>
<li>Finds the nearest index and quantized embeddings for a batch of <code>x</code>
<ul>
<li>Call <code>find_nearest_codes</code> to get <code>indices</code> and <code>quantized</code></li>
</ul>
</li>
<li>Applies the gradient estimator to get <code>quantized_st</code>
<ul>
<li>This will be used for gradient backprop to the encoder later</li>
<li><code>apply_gradient_estimator</code> either uses the straight through or rotation method</li>
</ul>
</li>
<li>Compute losses:
<ul>
<li><code>codebook_loss</code> is the MSE loss between <code>x.detach()</code> and <code>quantized</code>
<ul>
<li>We want to pull the codebook embeddings toward <code>x</code></li>
</ul>
</li>
<li><code>commitment_loss</code> is the MSE loss between <code>x</code> and <code>quantized.detach()</code>
<ul>
<li>We want to pull encoder output toward codebook embeddings</li>
</ul>
</li>
<li><code>loss = codebook_loss + beta * commitment_loss</code></li>
</ul>
</li>
<li>Everything is packaged into <code>QuantizationOutput</code> and returned</li>
<li><code>self.update_usage</code> is also called:
<ul>
<li>Updates counts of which indices were the nearest to <code>x</code></li>
<li>Updates the number of training steps</li>
</ul>
</li>
</ul>
</li>
<li>Straight through
<ul>
<li>The straight through gradient estimator simply returns <code>x + (quantized - x).detach()</code></li>
<li>Essentially, the embeddings passed forward is <code>quantized</code></li>
<li>But the vector used for gradient backprop is <code>x</code> (hence straight-through back to the encoded <code>x</code>)</li>
<li>This is a naive method but works well enough</li>
</ul>
</li>
<li>Rotation
<ul>
<li>The problem with the straight through estimator is that we use <code>quantized</code> for the forward pass but use <code>x</code> for the backward pass
<ul>
<li>This can be problematic especially if <code>q</code> and <code>x</code> are far apart</li>
</ul>
</li>
<li>The rotation idea is to apply a rotation to <code>x</code> until it aligns with <code>q</code>
<ul>
<li>Since the rotation is differentiable, we get better gradients back to <code>x</code></li>
</ul>
</li>
<li>We compute (to check later):
<ul>
<li>Let <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mord">/∣∣</span><span class="mord mathnormal">x</span><span class="mord">∣∣</span></span></span></span></li>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3744em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8544em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣∣</span><span class="mord mathnormal mtight">u</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mord mtight">∣∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> is the halfway vector between <code>u</code> and <code>q</code></li>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mopen">⟨</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">⟩</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mopen">⟨</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">u</span><span class="mclose">⟩</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span></li>
</ul>
</li>
</ul>
</li>
<li><code>reset_unused_codes</code>
<ul>
<li>Look up <code>self.usage_count</code> to find unused indices (used <code>0</code> times)</li>
<li>Take the current batch of encoded data, and randomly select them to become the new codebook vectors</li>
<li>This makes it likely for them to be used in the next forward pass since they correspond to actual encoder outputs</li>
<li>All usage counters are reset after this</li>
</ul>
</li>
</ul>
<h4 id="rqvae"><a class="header" href="#rqvae">RQVAE</a></h4>
<p>The RQVAE class now assembles multiple <code>VectorQuantizer</code> into the actual VAE to create semantic IDs.</p>
<p>At initialization:</p>
<ul>
<li><code>self.encoder</code>: a simple MLP that shrinks the input embedding down to the codebook dimension
<ul>
<li>In this code, we go from <code>1024 -&gt; 512 -&gt; 256 -&gt; 128 -&gt; 32</code></li>
</ul>
</li>
<li><code>self.decoder</code>: a simple MLP that goes backward from quantized vector up to embedding dimension
<ul>
<li>In this code, the decoder dims are just the reversed of the encoder dims</li>
<li>So we go from <code>32 -&gt; 128 -&gt; 256 -&gt; 512 -&gt; 1024</code></li>
</ul>
</li>
<li>Both encoder and decoder are wrapped in <code>nn.Sequential</code></li>
<li><code>self.vq_layers</code> contains the <code>VectorQuantizer</code>s
<ul>
<li>It is an <code>nn.ModuleList</code> of <code>3</code> <code>VectorQuantizer</code>s</li>
</ul>
</li>
<li><code>forward</code>: the main magic of this class
<ul>
<li>First we encode input item embedding <code>z = self.encode(x)</code></li>
<li>Also init <code>residual = z</code></li>
<li>Init <code>quantized_out = torch.zeros_like(z)</code>
<ul>
<li>The quantization output will be the sum of</li>
</ul>
</li>
<li>Now we run a for loop through the vector quantizer layers:
<ul>
<li>First we compute the quantization output for this level (which contains the mapped ID for this level etc.)
<ul>
<li><code>vq_output: QuantizationOutput = vq_layer(residual)</code></li>
</ul>
</li>
<li>Then we update the residual by subtracting the nearest codebook vector
<ul>
<li><code>residual -= vq_output.quantized.detach()</code></li>
</ul>
</li>
<li>We accumulate the codebook vectors (with gradients) into <code>quantized_out</code>
<ul>
<li><code>quantized_out += vq_output.quantized_st</code></li>
<li>Recall that the final representation passed to the decoder is <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2809em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>We also accumulate the loss for each layer
<ul>
<li><code>vq_loss += vq_output.loss</code></li>
<li>This is the codebook + commitment loss, reconstruction loss comes later</li>
</ul>
</li>
</ul>
</li>
<li>Finally we get the total loss
<ul>
<li>Compute the reconstruction loss
<ul>
<li><code>x_recon = self.decode(quantized_out)</code></li>
<li><code>recon_loss = F.mse_loss(x_recon, x)</code></li>
<li><code>loss = recon_loss + vq_loss</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>encode_to_semantic_ids</code>: encodes an item embedding <code>x</code> to an integer tensor representing its semantic ID</li>
<li><code>decode_from_semantic_ids</code>: decodes an integer tensor <code>semantic_ids</code> by looking up the codebook, summing up the levels and passing back into the <code>decoder</code></li>
<li><code>kmeans_init</code>
<ul>
<li>Runs kmeans on one batch of embeddings to initialize the codebook vectors</li>
<li>Runs kmeans to get <code>256</code> centroid vectors</li>
<li>Copies these vectors into the codebook directly</li>
<li>Process layer by layer</li>
</ul>
</li>
</ul>
<h3 id="finetune_qwen3_8b_vocabpy"><a class="header" href="#finetune_qwen3_8b_vocabpy">finetune_qwen3_8b_vocab.py</a></h3>
<p>This script performs <span style="color:orange">Stage 1</span> of the qwen fine-tuning. It focuses on extending the vocabulary to include new semantic ID tokens and trains embeddings for these new tokens.</p>
<h4 id="finetuneconfig"><a class="header" href="#finetuneconfig">FineTuneConfig</a></h4>
<p>Dataclass containing config for the training</p>
<ul>
<li><code>model_name</code>: <code>unsloth/Qwen3-8B</code>
<ul>
<li><span style="color:orange">Qn:</span> Not instruction fine tuned?</li>
</ul>
</li>
<li><code>load_in_4bit</code>: Set to <code>False</code> for embedding training</li>
<li><code>load_in_8bit</code>: Set to <code>False</code></li>
<li><code>num_proc</code>: <code>32</code></li>
<li><code>enable_thinking</code>: <code>False</code> we don't need thinking mode</li>
<li><code>extend_vocabulary</code>: <code>True</code></li>
<li><code>codebook_levels</code>: <code>4</code></li>
<li><code>codebook_size</code>: <code>256</code></li>
<li><code>num_semantic_tokens</code>: <code>1024</code></li>
<li><code>system_prompt</code>: see below</li>
<li><code>max_training_samples</code>: <code>32000</code> limit for training embedding</li>
<li><code>learning_rate</code>: <code>1e-3</code></li>
<li><code>batch_size</code>: <code>32</code></li>
<li><code>max_steps</code>: <code>1000</code></li>
</ul>
<p>The system prompt is as follows:</p>
<blockquote>
<p>"You are a helpful AI assistant that understands and works with semantic IDs for product recommendations. Semantic IDs are hierarchical identifiers in the format <code>&lt;|sid_start|&gt;&lt;|sid_105|&gt;&lt;|sid_307|&gt;&lt;|sid_705|&gt;&lt;|sid_769|&gt;&lt;|sid_end|&gt;</code> that encode product relationships and categories. /no_think"</p>
</blockquote>
<h4 id="extend_tokenizer"><a class="header" href="#extend_tokenizer">extend_tokenizer</a></h4>
<p><code>extend_tokenizer(model, tokenizer, config: FineTuneConfig)</code> adds semantic ID tokens to the tokenizer using Unsloth's <code>add_new_tokens</code>.</p>
<ul>
<li>Note that the vocab size affects two places:
<ul>
<li><code>model.get_input_embeddings().weight</code>: the input embeddings</li>
<li><code>model.get_output_embeddings().weight</code>: the language model head which predicts the next token</li>
</ul>
</li>
<li>First, we make sure that the vocab size of the tokenizer matches the vocab size of both the input and output embeddings
<ul>
<li>We need to call <code>model.resize_token_embeddings</code> to get the model embedding sizes to match the <code>tokenizer</code></li>
<li>This is because the model embeddings are padded to be multiples of <code>128</code> for CUDA optimization reasons</li>
</ul>
</li>
<li>Next, we add new tokens using <code>unsloth.add_new_tokens</code>:
<ul>
<li>Special tokens of <code>&lt;|rec|&gt;</code>, <code>&lt;|sid_start|&gt;</code>, <code>&lt;|sid_end|&gt;</code></li>
<li>Semantic IDs of <code>&lt;|sid_0|&gt;</code> to <code>&lt;|sid_1023|&gt;</code></li>
</ul>
</li>
</ul>
<h4 id="prepare_model"><a class="header" href="#prepare_model">prepare_model</a></h4>
<p>Prepares the model for training with some additional checks:</p>
<ul>
<li>Freezes gradients for all parameters</li>
<li>Unfreezes only the weights for the <code>model.get_input_embeddings()</code> and <code>model.get_output_embeddings()</code></li>
<li>Checks the trainable parameter %</li>
</ul>
<h4 id="load_sid_dataset"><a class="header" href="#load_sid_dataset">load_sid_dataset</a></h4>
<p>Loads the semantic IDs training dataset:</p>
<ul>
<li>Checks if there are texts like <code>&lt;|sid_start|&gt;</code> to make sure processing is correct</li>
<li>Applies chat template to the rows (but keeps as text)</li>
</ul>
<p>There are 5 distinct categories of training data:</p>
<ul>
<li><span style="color:orange">SemanticID -&gt; text</span>:
<ul>
<li><strong>Input</strong>: "Product <code>&lt;|sid_start|&gt;...&lt;|sid_end|&gt;</code> has title:"</li>
<li><strong>Output</strong>: "Super Mario Bros"</li>
<li><strong>Variations</strong>: ID to title, description, category, features or full context</li>
</ul>
</li>
<li><span style="color:orange">Text -&gt; SemanticID</span>:
<ul>
<li><strong>Input</strong>: "The product Super Mario Bros has SemanticID:"</li>
<li><strong>Output</strong>: "<code>&lt;|sid_start|&gt;...&lt;|sid_end|&gt;</code>"</li>
<li><strong>Variations</strong>: Similar variations to above</li>
</ul>
</li>
<li><span style="color:orange">Sequential Recommendation</span>:
<ul>
<li><strong>Input</strong>: "Based on recent purchases etc., next item:"</li>
<li><strong>Output</strong>: "<code>&lt;|sid_start|&gt;...&lt;|sid_end|&gt;</code>"</li>
<li><strong>Variations</strong>: Various sequence lengths of 2, 3, or 5 items.</li>
</ul>
</li>
<li><span style="color:orange">Semantic Understanding</span>:
<ul>
<li><strong>Input</strong>: "Products starting with <code>&lt;|sid_start|&gt;&lt;|sid_64|&gt; are typically:</code></li>
<li><strong>Output</strong>: "Nintendo switch games"</li>
<li><strong>Variations</strong>: Prefix to category, prefix to examples, similar items.</li>
</ul>
</li>
<li><span style="color:orange">Multi-hop Reasoning</span>:
<ul>
<li><strong>Input</strong>: "A user who bought <code>&lt;|sid_a|&gt;</code> might also buy:"</li>
<li><strong>Output</strong>: "<code>&lt;|sid_b|&gt;</code></li>
<li><strong>Variations</strong>: Co-purchase patterns.</li>
</ul>
</li>
</ul>
<h4 id="datainspectioncallback"><a class="header" href="#datainspectioncallback">DataInspectionCallback</a></h4>
<p>Used to inspect training data and tokenization at each training step, by simply logging them to console.</p>
<p>Patterns:</p>
<ul>
<li><code>DataInspectionCallback</code> subclasses <code>transformers.TrainerCallback</code></li>
<li><code>on_train_begin(self, args, state, control, **kwargs)</code>:
<ul>
<li>Checks the first batch of <code>train_dataloader</code></li>
<li>Checks batch keys</li>
<li>Check shape of <code>batch['input_ids']</code></li>
<li>Check shape of <code>batch['attention_mask']</code></li>
<li>Check tokens and decoded of first row etc.</li>
</ul>
</li>
<li><code>on_log(self, args, state, control, logs=None, **kwargs)</code>:
<ul>
<li>Only runs if <code>state.global_step % args.logging_steps == 0</code></li>
<li>Check number of SID tokens</li>
<li>Decode first example and check</li>
</ul>
</li>
</ul>
<h4 id="embeddingmonitorcallback"><a class="header" href="#embeddingmonitorcallback">EmbeddingMonitorCallback</a></h4>
<p>This callback aims to check how our embeddings are shifting over time.</p>
<ul>
<li>At initialization (or <code>on_train_begin</code>), we copy the state of the initial embeddings and clone detach them</li>
<li>At each step, we compute the mean of the absolute difference between the current embeddings and the initial or state of embeddings from the previous step</li>
<li>We also compute the per level codebook vector means etc.</li>
<li>These are all logged to <code>wandb</code></li>
</ul>
<h4 id="semanticidgenerationcallback"><a class="header" href="#semanticidgenerationcallback">SemanticIDGenerationCallback</a></h4>
<p>This is a qualitative check to answer the question "If I ask the model for a recommendation right now, does it use the semantic ID tokens or does it just output plain text?"</p>
<ul>
<li>A fixed set of test cases are used</li>
<li>The test cases are <code>apply_chat_template</code>, then passed into the <code>tokenizer</code>, then <code>model.generate</code> and <code>model.decode</code></li>
<li>The messages are checked whether successful (SemanticIDs generated) and success rate is tracked</li>
<li>Actual completion examples are logged into wandb as well</li>
</ul>
<h4 id="train_embeddings"><a class="header" href="#train_embeddings">train_embeddings</a></h4>
<p>The main method. Essentially we are just using unsloth <code>SFTTrainer</code> to do the training.</p>
<p>First, we set up <code>trl.SFTConfig</code> with a lot of the configuration we previously defined.</p>
<ul>
<li>Note that <code>dataset_text_field="text"</code></li>
<li><code>report_to="wandb"</code></li>
</ul>
<p>The trainer <code>trl.SFTTrainer</code> is initialized with the model, tokenizer, datasets, config and callbacks.</p>
<p>Then, <code>trainer.train()</code> is called.</p>
<p>Note that the model and tokenizer are initialized using <code>unsloth.FastLanguageModel</code> to use unsloth's optimized triton kernels.</p>
<h3 id="finetune_qwen3_8b_fullpy"><a class="header" href="#finetune_qwen3_8b_fullpy">finetune_qwen3_8b_full.py</a></h3>
<p>The code is structurally very similar to the vocab finetuning run. The difference is that we are doing full training, so we unfreeze all parameters. Consequently, the learning rate needs to be much lower at <code>2e-5</code>.</p>
<ul>
<li>Load the model from stage 1, namely <code>models/qwen3_8b_vocab_extended/final</code></li>
<li>A lot of the script focuses on the callbacks to evaluate recommendation quality</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../talks/han_2025_grpo.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../articles/cherny_2025_vibe_coding.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../talks/han_2025_grpo.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../articles/cherny_2025_vibe_coding.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
