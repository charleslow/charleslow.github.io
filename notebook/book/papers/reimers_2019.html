<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Reimers 2019 - Sentence-BERT - Chux&#x27;s Notebook</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../katex.min.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../intro.html">Introduction</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.</strong> Recommender Systems</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../gradient_boosting.html"><strong aria-hidden="true">1.1.</strong> Gradient Boosting</a></li><li class="chapter-item expanded "><a href="../tfidf.html"><strong aria-hidden="true">1.2.</strong> TF-IDF</a></li><li class="chapter-item expanded "><a href="../cross_encoders.html"><strong aria-hidden="true">1.3.</strong> Cross Encoders</a></li><li class="chapter-item expanded "><a href="../sentence_transformers.html"><strong aria-hidden="true">1.4.</strong> SentenceTransformers</a></li><li class="chapter-item expanded "><a href="../collab_filtering.html"><strong aria-hidden="true">1.5.</strong> Collaborative Filtering</a></li></ol></li><li class="chapter-item expanded "><a href="../ab_test/init.html"><strong aria-hidden="true">2.</strong> AB Testing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../ab_test/examples.html"><strong aria-hidden="true">2.1.</strong> Examples</a></li><li class="chapter-item expanded "><a href="../ab_test/power_analysis.html"><strong aria-hidden="true">2.2.</strong> Power Analysis</a></li></ol></li><li class="chapter-item expanded "><a href="../llm/llm.html"><strong aria-hidden="true">3.</strong> LLMs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../llm/fine_tuning.html"><strong aria-hidden="true">3.1.</strong> Fine-tuning</a></li><li class="chapter-item expanded "><a href="../llm/useful_models.html"><strong aria-hidden="true">3.2.</strong> Useful Models</a></li><li class="chapter-item expanded "><a href="../llm/encoder_vs_decoder.html"><strong aria-hidden="true">3.3.</strong> Encoder vs Decoder</a></li></ol></li><li class="chapter-item expanded "><a href="../misc.html"><strong aria-hidden="true">4.</strong> Miscellaneous</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../misc/bradley-terry.html"><strong aria-hidden="true">4.1.</strong> Bradley-Terry Model</a></li><li class="chapter-item expanded "><a href="../misc/wsl-setup.html"><strong aria-hidden="true">4.2.</strong> Setting up WSL</a></li><li class="chapter-item expanded "><a href="../misc/to-read.html"><strong aria-hidden="true">4.3.</strong> To Read</a></li><li class="chapter-item expanded "><a href="../misc/packages.html"><strong aria-hidden="true">4.4.</strong> Packages</a></li><li class="chapter-item expanded "><a href="../misc/skills.html"><strong aria-hidden="true">4.5.</strong> Skills</a></li></ol></li><li class="chapter-item expanded "><a href="../identities.html"><strong aria-hidden="true">5.</strong> Identities</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../identities/sigmoid.html"><strong aria-hidden="true">5.1.</strong> Sigmoid</a></li><li class="chapter-item expanded "><a href="../identities/statistics.html"><strong aria-hidden="true">5.2.</strong> Statistics</a></li></ol></li><li class="chapter-item expanded "><a href="../papers.html"><strong aria-hidden="true">6.</strong> Papers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../papers/weinberger_2009.html"><strong aria-hidden="true">6.1.</strong> Weinberger 2009 - Hashing for Multitask Learning</a></li><li class="chapter-item expanded "><a href="../papers/burges_2010.html"><strong aria-hidden="true">6.2.</strong> Burges 2010 - RankNET to LambdaMART</a></li><li class="chapter-item expanded "><a href="../papers/schroff_2015.html"><strong aria-hidden="true">6.3.</strong> Schroff 2015 - FaceNET</a></li><li class="chapter-item expanded "><a href="../papers/schnabel_2016.html"><strong aria-hidden="true">6.4.</strong> Schnabel 2016 - Recs as Treatments</a></li><li class="chapter-item expanded "><a href="../papers/guo_2017.html"><strong aria-hidden="true">6.5.</strong> Guo 2017 - DeepFM</a></li><li class="chapter-item expanded "><a href="../papers/hamilton_2017.html"><strong aria-hidden="true">6.6.</strong> Hamilton 2017 - GraphSAGE</a></li><li class="chapter-item expanded "><a href="../papers/ma_2018.html"><strong aria-hidden="true">6.7.</strong> Ma 2018 - Entire Space Multi-Task Model</a></li><li class="chapter-item expanded "><a href="../papers/reimers_2019.html" class="active"><strong aria-hidden="true">6.8.</strong> Reimers 2019 - Sentence-BERT</a></li><li class="chapter-item expanded "><a href="../papers/he_2020.html"><strong aria-hidden="true">6.9.</strong> He 2020 - LightGCN</a></li><li class="chapter-item expanded "><a href="../papers/lewis_2020.html"><strong aria-hidden="true">6.10.</strong> Lewis 2020 - Retrieval Augmented Generation</a></li><li class="chapter-item expanded "><a href="../papers/gao_2021.html"><strong aria-hidden="true">6.11.</strong> Gao 2021 - SimCSE</a></li><li class="chapter-item expanded "><a href="../papers/weng_2021.html"><strong aria-hidden="true">6.12.</strong> Weng 2021 - Contrastive Representation Learning</a></li><li class="chapter-item expanded "><a href="../papers/dao_2022.html"><strong aria-hidden="true">6.13.</strong> Dao 2022 - Flash Attention</a></li><li class="chapter-item expanded "><a href="../papers/tunstall_2022.html"><strong aria-hidden="true">6.14.</strong> Tunstall 2022 - SetFit</a></li><li class="chapter-item expanded "><a href="../papers/rafailov_2023.html"><strong aria-hidden="true">6.15.</strong> Rafailov 2023 - Direct Preference Optimization</a></li><li class="chapter-item expanded "><a href="../papers/blecher_2023.html"><strong aria-hidden="true">6.16.</strong> Blecher 2023 - Nougat</a></li><li class="chapter-item expanded "><a href="../papers/borisyuk_2024.html"><strong aria-hidden="true">6.17.</strong> Borisyuk 2024 - GNN at LinkedIn</a></li><li class="chapter-item expanded "><a href="../papers/liu_2023.html"><strong aria-hidden="true">6.18.</strong> Liu 2023 - Meaning Representations from Trajectories</a></li></ol></li><li class="chapter-item expanded "><a href="../nlp_course/intro.html"><strong aria-hidden="true">7.</strong> NLP Course</a></li><li class="chapter-item expanded "><a href="../database_course/intro.html"><strong aria-hidden="true">8.</strong> Database Course</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../database_course/lecture01.html"><strong aria-hidden="true">8.1.</strong> Lecture 1</a></li><li class="chapter-item expanded "><a href="../database_course/lecture02.html"><strong aria-hidden="true">8.2.</strong> Lecture 2</a></li><li class="chapter-item expanded "><a href="../database_course/lecture03.html"><strong aria-hidden="true">8.3.</strong> Lecture 3</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Chux&#x27;s Notebook</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="reimers-2019---sentence-bert"><a class="header" href="#reimers-2019---sentence-bert">Reimers 2019 - Sentence-BERT</a></h1>
<p><a href="https://ar5iv.labs.arxiv.org/html/1908.10084">Link to paper</a>.</p>
<p>Sentence-BERT (or SBERT) was one of the first papers to suggest a way to fine-tune BERT models to generate useful embeddings that can be used for search / retrieval.</p>
<p>Prior to SBERT, BERT models were mainly used for sentence pair regression tasks by passing two sentences into the transformer network and adding a classification head on top to produce a float value. We can call this the <span style="color:orange">cross-encoder</span> approach. In other words, researchers only cared about the final prediction and did not make use of the embeddings, or the final vector representation of the inputs. This approach is suitable for reranking a small number of documents but not for nearest neighbour search in a corpus with millions of documents.</p>
<p>Naively, one can element-wise average the BERT embeddings at the final layer to produce a vector representation of the text. This vector representation can then be used for nearest neighbour search or clustering. However, because BERT was not explicitly trained for this objective, it results in rather bad sentence embeddings, often worse than GloVe embeddings.</p>
<h2 id="method"><a class="header" href="#method">Method</a></h2>
<p>The SBERT paper presents 3 different training objectives, all of which perform well on embedding similarity tasks. The choice of objective depends on the dataset:</p>
<ol>
<li>
<p><span style="color:orange">Classification objective</span>. This is for tasks where the objective is to predict a label given two sentences A, B. We pass each sentence into the BERT network and a pooling layer to get two vector representations, <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span> and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span>. The pooling layer can be (i) take the <code>[CLS]</code> token embedding, (ii) take the element-wise mean or (iii) take the element-wise max. We then create a concatenated vector <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span> which is fed into a softmax classifer. The network is trained using cross-entropy loss.</p>
<p>Note that this siamese approach (where each sentence is passed into the same network) differs a little from the typical cross-encoder approach, where the sentences are concatenated as a <span style="color:orange">string</span> with the <code>[SEP]</code> token before passed into the network. The latter approach is presumably more powerful because the attention mechanism can attend to all pairwise relationships</p>
</li>
<li>
<p><span style="color:orange">Regression objective</span>. This is for tasks where the objective is to predict a float given two sentences A, B. Given the vectors <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span> and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span>, the cosine similarity is simply taken to generate a float between <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mord">1</span></span></span></span> and <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>. The cosine similarity is then compared with the actual float value using mean-squared error to generate a loss.</p>
</li>
<li>
<p><span style="color:orange">Triplet objective</span>. This is for tasks where each data point is a triplet (anchor sentence <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>, positive sentence <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span>, negative sentence <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>). We then minimize the loss function, where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> is the margin:
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">∣∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">∣∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p>
</li>
</ol>
<h2 id="ablation"><a class="header" href="#ablation">Ablation</a></h2>
<ol>
<li><span style="color:orange">Pooling strategy</span>. Using <code>[CLS]</code> or mean seems to be largely similar. The authors saw some degradation using max for the regression objective.</li>
<li><span style="color:orange">Concatenation</span>. For the classification objective, the concatenation strategy makes some difference. In particular, using <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span> yields <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.66</span></span></span></span> but <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span> yields <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.81</span></span></span></span>. Thus the element-wise difference is important in yielding useful embeddings, probably because it can be used to push similar sentences together and dissimilar sentences apart. The authors also found that adding element-wise multiplication <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> does not help.</li>
</ol>
<h2 id="takeaway"><a class="header" href="#takeaway">Takeaway</a></h2>
<p>It is interesting that the classification objective, which is close to a cross-encoder framework, is also able to learn useful embeddings by adding the difference operation <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">∣</span></span></span></span>. This suggests that we can train a cross encoder and simultaneously get useful embeddings for nearest neighbour retrieval at the same time.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../papers/ma_2018.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../papers/he_2020.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../papers/ma_2018.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../papers/he_2020.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
