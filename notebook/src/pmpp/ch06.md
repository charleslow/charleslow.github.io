# Performance Considerations

This chapter goes deeper into off chip memory (DRAM) and discusses memory coalescing and memory latency hiding. It also discusses thread granularity coarsening.

## Memory Coalescing

One of the most important factors in CUDA kernel performance is accessing data in the global memory. This chapter further discusses memory coalescing techniques to move data between global memory and shared memory or registers efficiently.

Global memory is implemented with DRAM:
- Access latency to a bit in DRAM is relatively slow (tens of nanoseconds)
- Because of this slowness, DRAM is designed to use parallelism to increase the memory access throughput

Each time a DRAM location is accessed, a range of consecutive locations nearby are accessed. Once detected by the sensors, the data from all these locations can be transferred to the processor.
- These consecutive locations assessed is known as <<DRAM bursts>>
- Making use of these consecutive and simultaneous accesses can allow us to improve memory access throughput

When a warp of threads is launched, all threads in the warp execute the same instruction at the same point in time. The hardware detects whether the threads are accessing consecutive memory locations.
- Thus, the best access pattern is for threads in a warp to access consecutive data locations in global memory
- We say that the hardware <<coalesces>> the accesses into a consoliated access to consecutive DRAM locations

Recall that multidimensional data are linearized according to the row-major convention and stored into memory space. The read accesses are coalesced if consecutive threads in a warp are assigned to consecutive memory elements in this row major storage format.

### Row Major

Consider the following matrix multiplication code in which each thread computes one element of the output matrix `P` without tiling:

```cpp
unsigned int row = blockIdx.y*blockDim.y + threadIdx.y;
unsigned int col = blockIdx.x*blockDim.x + threadIdx.x;
if (row < Width && col < Width) {
    float Pvalue = 0.0f;
    for (unsigned int k = 0; k  < Width; ++k) {
        Pvalue += N[row*Width + k]*M[k*Width + col];
    }
    P[row*Width + col] = Pvalue;
}
```

Notice that memory accesses to `M` are coalesced because:
- The index of `M` is `k * Width + col`
- `k` and `Width` have the same values across all threads in a warp
- `col = blockIdx.x * blockDim.x + threadIdx.x`, which means that consecutive threads (with incrementing `threadIdx.x`) will access consecutive elements

### Column Major

Now suppose the matrix is stored in column major format instead. Now the code adjusts to the following:

```cpp
unsigned int row = blockIdx.y*blockDim.y + threadIdx.y;
unsigned int col = blockIdx.x*blockDim.x + threadIdx.x;
if (row < Width && col < Width) {
    float Pvalue = 0.0f;
    for (unsigned int k = 0; k  < Width; ++k) {
        Pvalue += N[row*Width + k]*M[col*Width + k];
    }
    P[row*Width + col] = Pvalue;
}
```

Observe that the index to `M` changed from `k*Width + col` to `col*Width + k`. This is because in the column major format, elements that were previously in adjacent columns to each other are now in adjacent rows. Hence the indexing to `M` is swapped to reflect this.

Now we can see that we have lost the coalesced access to `M`:
- The indexing to `M` is now `col*Width + k`
- Consecutive threads still have consecutive values of `col`
- But `col` is now multiplied by `Width`, meaning that consecutive threads access values that are `Width` apart, which can be very far

### Corner Turning

This is a method to create coalesced memory accesses even when the computation is not naturally amenable to it. The idea is to read from global memory into shared memory in a coalesced manner, then do the unfavourable access pattern from the cheap shared memory instead.

Suppose we are in the tiled matrix multiplication example. First, we do a coalesced read from `M` into `Mds` for our tile by swapping the row and column. The elements can be arranged in `Mds` in either row or column major format, it does not matter. Then, we can do fast reads from `Mds` to do the matrix multiplication for our tile.

## Hiding Memory Latency

DRAM bursting is a form of parallel organization: multiple locations are accessed in the DRAM core array in parallel. There are two more ways of parallel organization: <<banks>> and <<channels>>. A single processor may have `8` channels. Each channel has a bus that accesses many banks.

The bus transfers data from the banks back to the processor. The data transfer bandwidth of the bus is defined by its width and clock frequency. Modern DDR buses perform two data transfers per clock cycle.
- e.g. A 64-bit DDR bus with clock frequency of 1 GHz has a bandwidth of `8B * 2 * 1 = 16GB/s`. 

For each channel, we need many banks in order to fully utilize the data transfer bandwidth of the bus. Why?
- Recall earlier that the data access latency is slow, because it takes time for cells to share their stored charge with the sensing amplifier
- Once the sensing amplifier has completed its work, the burst data is delivered through the bus (this part is relatively much faster)
- If we only have a single bank, there is a long latency waiting for the sensing amplifier while the bus sits idle
- If we have multiple buses, we can trigger multiple read accesses to different banks in close succession in an async manner
- When each bank has been sensed, the delivery of burst data can be tightly packed and fill up the idle time

In general, if the ratio of `cell array access latency / data transfer time = R`, then we need to have at least `R + 1` banks to fully utilize the data transfer bandwidth of the bus. 
- Note that we need more than `R` because of <<bank conflict>>. Since each bank can only serve one data access at a time (each bank can store multiple data points), if multiple threads need to access the same bank for data, there is a conflict. So having more banks help to reduce the probability of bank conflict

There is an important connection between occupancy and parallel access to DRAM data. As we saw earlier, maximizing occupancy ensures that we have enough threads on the SMs to hide long memory access latencies. In this section, we see another distinct advantage of maximizing occupancy - having many threads access different memory locations allows us to fully utilize the memory transfer bandwidth of the buses. That is, if the memory accesses are well distributed across channels and banks.

Practically, the distribution of data across channels and banks is in an <<interleaved data distribution>>. Suppose we have a toy example:
- `4` channels
- `2` banks per channel
- Each bank stores `2` consecutive floats

Now suppose we are doing a tiled matrix multiplication between two `4 x 4` matrices `M @ N`, where we set the tile width to `2`.

The storage of `M` is like so (referring to the linearized index):
- Channel 0
    - Bank 0: `M[0], M[1]`
    - Bank 1: `M[8], M[9]`
- Channel 1
    - Bank 0: `M[2], M[3]`
    - Bank 1: `M[10], M[11]`
- Channel 2
    - Bank 0: `M[4], M[5]`
    - Bank 1: `M[12], M[13]`
- Channel 3
    - Bank 0: `M[6], M[7]`
    - Bank 1: `M[14], M[15]`

As we can see, consecutive elements of our data arrays is first put into `bank 0` of all the channels. It then moves on to `bank 1` of all the channels. If there is more data, we will wrap back around to `bank 0` again. The breadth-first distribution across channels helps ensure that even when the amount of data is small, we have good distribution of data across channels and banks.

Now for the matrix multiplication, we can verify that in phase 0 of the tiled matrix multiplication, these are the elements of `M` requested by each block:
- `Block 0,0`: `M[0], M[1], M[4], M[5]`
- `Block 0,1`: `M[0], M[1], M[4], M[5]`
- `Block 1,0`: `M[8], M[9], M[12], M[13]`
- `Block 1,1`: `M[8], M[9], M[12], M[13]`

This is a bit tedious so will not elaborate on how these were derived. But basically, we can see that we access both banks of channels 0 and 2 for phase 1. So we are utilizing 50% of the channels. With multiplication of larger matrices, we can fully utilize all channels and buses.

## Thread Coarsening

So far, we have focused on kernels where work is parallelized across all threads at the finest granularity. For e.g. in the matrix multiplication example, each thread is assigned one element in the output matrix `P`. The advantage of parallelizing at the finest granularity is that it enhances <<transparent scalability>> - if the hardware has enough resources to run all the work in parallel, then we have maximized the parallelism.

However, if the hardware does not have enough resources to run all the work at once, then a queue is formed up, and the parallel work becomes sequential. In this case, there is a price to be paid for such aggressive parallelism, such as:
- redundant loading of the same data by different blocks
- redundant work
- syncrhonization overhead

When the threads are executed in parallel, this price is often well worth it. But if the hardware ends up serializing the work as a result of insufficient resources, this price may have been paid unnecessarily. It may thus be worthwhile to reduce the amount of parallelism by having each thread do more work, in exchange for reducing the price that we pay.

One such technique is called <<thread coarsening>>. We can see a good example in the tiled matrix multiplication example. For calculating a given tile in the output matrix `P`, each thread block must load its own copy of the input tiles of matrix `M`. However, this means that the same tile of input matrix `M` will be loaded redundantly by different thread blocks and incurring access to global memory.

More concretely, consider that we are computing rows of `P` from `P[i, :]` to `P[j, :]`. 
- One thread block may be computing `P[i, k1] to P[j, k2]`
- One thread block may be computing `P[i, k2] to P[j, k3]`
- But notice that to do this computation, they all have to access `M[i, k1] to M[j, k2]` in one of their phases
- It may make more sense for a single thread block to compute a few adjacent `P` tiles at once, and thus load this slice of matrix `M` only once and reduce redundant memory accesses

Here we can dive into the thread coarsening matrix multiplication kernel:

```c
```
