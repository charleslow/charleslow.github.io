# Memory Architecture

This chapter focuses on the memory architecture of GPUs and how to organize and position data for efficient access.

Consider the matrix multiplication code back in chapter 3. This performs a for loop to compute the dot product between a row of M and a column of N. 

```c
for (int k = 0; k < Width; ++k) {
    Pvalue += M[row*Width + k] * N[k*Width + col];
}
```

In every iteration of the loop:
- We perform two global memory accesses (for M and N respectively)
- We perform one floating point multiplication and one floating point addition
- The ratio of floating point operation (FLOP) to bytes accessed is `2 FLOP / 8 bytes = 0.25 FLOP/B`. This is called <<arithmetic intensity>> or <<computational intensity>> in the literature.

The compute to global memory access has implications for the performance of a GPU kernel.
- An A100 has peak global memory bandwidth of `1555 GB/s`. With `0.25 FLOP/B`, we can get `1555 GB/s * 0.25 FLOP/B = 389 giga FLOPs/s (GLOPS)` number of single precision floating point operations.
- But A100 has a cap of `19,500 GLOPS`, so we are only using `2%` of the compute capacity!
- We are severely limited by memory accesses, and this type of program is called `memory-bound` programs

## Roofline Model

The roofline model provides a visual representation of the performance of an application relative to hardware limits. 

| ![Roofline model illustrating bandwidth and compute ceilings](../images/pmpp/ch_05_roofline_model.png) |
|:--:|
| **Roofline model for analyzing GPU efficiency** |

The axes meanings:
- `x-axis` is the computational intensity measured in `FLOP/B`. This quantity is determined by how the application is written.
- `y-axis` is the computational throughput, i.e. obtained by multiplying the computational intensity by the maximum memory bandwith of our hardware device (a fixed constant)
    - The <<peak throughput>> horizontal line is the hard cap of the hardware on the peak throughput it can support

The sloping line labelled <<peak bandwidth>> represents the hardward limit of the memory bandwidth. The slope is obtained by multipling the current position on the x-axis by the maximum memory bandwidth (say `1555GB/s` in the example above), which gives us a slope. We cannot have any point above this line because it would require a memory bandwidth that exceeds the hardware limits.

A point on the plot represents an application with its computational intensity (FLOPS / byte accessed) and computational throughput (FLOPS / s). Note that in real applications, these are measured using programs like NVIDIA NSight using hardware counters. The position of the point relative to the lines tells us its efficiency:
- Points can only be below the two lines, as we cannot exceed hardware limits
- Points closer to the lines are more efficient, as we are nearing hardware limits

Based on the diagram:
- Point $A_3$ is compute bound - it is already quite computationally intensive and near the peak computational throughput, so we need a more powerful GPU to improve it further.
- Point $A_2$ is inefficient - neither memory nor compute bound. It can be improved by improving the memory bandwidth utilization.
- Point $A_1$ is memory bound - it is utilizing memory bandwidth effectively but the compute intensity can be improved. e.g. To fully utilize the `19,500 GLOPS` from the A100, we need to increase compute efficiency to `19,500 (GFLOP/s) / 1,555 (GB/s) = 12.5 FLOP/B`. This means that for each single precision floating point number accessed, we need to perform `50 FLOPS`! This can be achieved for certain applications like matrix multiplication.

## CUDA Memory Types

There are different memory types on a GPU with different uses:
- Registers
    - Registers are on-chip memory, which means very short latency, high bandwidth access
    - Highly limited number of registers per SMs, using too many in kernel code can result in low occupancy
    - Allocated to individual threads - each thread can only access its own registers
- Shared memory
    - Also on-chip memory, but it resides outside of the Processing Unit
    - Accessing data requires a memory load (as opposed to registers), so it is slower than registers, but the memory load is much faster than global memory as it is on-chip
    - Allocated to thread blocks - all threads in a block can access shared memory variables
- Global memory
    - All threads can access
    - Off-chip memory, so relatively high latency and low bandwidth access
- Local memory
    - Placed in global memory and has similar access latency
    - Not shared across threads
    - Each thread has its own section of global memory that it uses as its own private local memory that is private to the thread but cannot be allocated in registers
    - e.g. statically allocated arrays

Efficient GPU code requires us to make full use of registers whenever possible:
- Access bandwidth of registers is at least 2 orders of magnitude higher than global memory
- When a variable is stored in registers, its access no longer consume off-chip global memory bandwith, which is reflected as an increased compute to global memory access ratio
- Each access to registers involves fewer instructions than access to global memory due to built-in register operands.
    - e.g. `fadd r1, r2, r3` directly adds values in registers `r2, r3` and stores it into `r1`. There is no loading step
    - But if the first operand is in the global memory, we need to do an extra loading step
    ```
    load r2, r4, offset
    fadd r1, r2, r3
    ```
- The energy consumed for accessing a value in the register file is an order of magnitude lower than accessing the value from global memory.

## CUDA Syntax

We can declare variables and reside them in the intended type of memory based on our application design.

> **Scope**. Scope identifies the set of threads than can access a variable. If the scope is a single thread, a private version of the variable is created for each thread.
> 
> **Lifetime**. The lifetime is the portion of a program's execution duration when the variable is available, If the lifetime is within a grid's execution, it must be declared within the kernel function. When the kernel is executed multiple times, the value is not maintained across these invocations. If the lifetime is entire application, it must be declared outside of any function body.

<<Automatic Scalar Variables>>. All automatic scalar variables declared in the kernel and device functions are automatically placed into registers. This means that a private copy of that variable in generated for every thread that executes the kernel function.
- Scope: Thread
- Lifetime: Grid

<<Automatic array variables>>. These are array variables defined in kernel code, such as `float buf[128]`. Note that the array size must be specified at compile time. These are put into local memory (part of global memory), so access is very slow. In practice, these are seldom used.
- Scope: Thread
- Lifetime: Grid

<<Shared Memory variables>>. These are declared in the kernel function and put into shared memory. The scope of a shared variable is in a thread block, so all threads in a block see the same version of a shared variable (and can read and write to it). A private version of the variable is created for each block during kernel execution. Accessing shared variables from the shared memory is extremely fast and highly parallel, and it is a good way for threads in a block to communicate with each other.
- Declaration: `__device__ __shared__ int SharedVar;`
- Scope: Block
- Lifetime: Grid

<<Constant Memory variables>>. These are declared in the host code outside of any function body, and its values cannot be changed by kernel function code. The name is indicative: its usually used for global constants like model parameters or for things like a fixed convolution filter. Constant variables are stored in global memory but are cached for extremely fast and parallel access. The total size of constant variables in an application is limited to `65,536 bytes`.
- Declaration: `__device__ __constant__ int ConstVar;`
- Scope: Grid
- Lifetime: Application

<<Global Memory variables>>. These are placed in global memory where access is slow. The advantage of global memory is that it is accessible by all threads and the contents persist throughout the entire application. So it is a way for threads to collaborate across blocks. However, because there is no barrier synchronization method for threads across blocks, usually global variables are used to pass information from one kernel invocation to another kernel.
- Declaration: `__device__ int GlobalVar;`
- Scope: Grid
- Lifetime: Application

## Tiling

The intrinsic memory tradeoff in CUDA is that global memory is large but slow, and shared memory is small but fast. Hence a good strategy is to partition the data into subsets called tiles so that each tile fits into the shared memory. 

Consider the matrix multiplication example in chapter 3 where each thread computes one value of the output matrix `P = M dot N`, and thus access one row of `M` and one column of `N`.
- For brevity, define $M_{y,x}$ as `M[y*Width + x]`, $N_{y,x}$ as `N[y*Width + x]`, $P_{y,x}$ as `P[y*Width + x]`
- Suppose `Width=4` and our block size is `2 x 2`

Now observe that there are duplicate global memory accesses across threads in a block:
- $M_{0,0}$ is accessed for computing both $P_{0,0}$ and $P_{0,1}$
- $N_{0,0}$ is accessed for computing both $P_{0,0}$ and $P_{1,0}$
- And so on

Instead of accessing global memory twice, we could have accessed it once, loaded it into shared memory, and then have threads read from shared memory thereafter (almost negligible cost compared to accessing global memory). In fact, a given value (like $M_{0,0}$) will be accessed `block_size` times for a given block, so the larger our block size, the more efficient our memory access can be.

We are limited by the size of shared memory, so we need to further tile `M` and `N` matrices so that it will fit into shared memory. For simplicity, consider tiling `M` and `N` into `2 x 2` *tiles* also.

Our strategy will now be like so:
- Initialize `2 x 2` shared variables `Mds` and `Nds`
- Load a tile of `M` into `Mds`:
    - For e.g. the first tile would be $M_{0,0}, M_{0,1}, M_{1,0}, M_{1,1}$
- Load a tile of `N` into `Nds`:
    - For e.g. the first tile would be $N_{0,0}, N_{0,1}, N_{1,0}, N_{1,1}$
- Launch threads to accumulate partial dot product into thread-specific `Pvalue`:
    - Instead of reading from `M` and `N`, we read the required variables from `Mds` and `Nds` instead
    - e.g. $\text{thread}_{0,0}$ computes $\text{Pvalue}_{0,0} \pluseq \text{Mds}_{0,0} * \text{Nds}_{0,0} + \text{Mds}_{0,1} * \text{Nds}_{1,0}$
    - Note that this is a partial dot product for $\text{Pvalue}_{0,0}$, we need to accumulate to it again
- Move on to the next tile and accumulate to `Pvalue` again:
    - e.g. Second tile is $M_{0,2}, M_{0,3}, M_{1,2}, M_{1,3}$
    - e.g. Second tile is $N_{2,0}, N_{2,1}, M_{3,0}, M_{3,1}$

Note that with this tiling strategy, we will need to do this in multiple steps (or phases) for a given block. In general, we need `Width / TILE_WIDTH` phases to complete the matrix multiplication.

## A tiled matmul kernel

Now we can tackle the matmul kernel.

```c
# define TILE_WIDTH 16
__global__ void matrixMulKernel(float* M, float* N, float* P, int Width) {
    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x;    int by = blockIdx.y;
    int tx = threadIdx.x;   int ty = threadIdx.y;

    // Identify P[Row, Col] to work on
    int Row = by * TILE_WIDTH + ty;
    int Col = bx * TILE_WIDTH + tx;

    float Pvalue = 0;
    for (int ph = 0; ph < Width/TILE_WIDTH; ++ph) {
        // Load M, N into shared memory
        Mds[ty][tx] = M[Row*Width + ph*TILE_WIDTH + tx];
        Nds[ty][tx] = N[(ph*TILE_WIDTH + ty)*Width + Col];
        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            Pvalue += Mds[ty][k] * Nds[k][tx]
        }
        __syncthreads();
    }
    P[Row*Width + Col] = Pvalue;
}
```

Notes on the above:
- `Mds, Nds` are shared variables that are common for all threads in a block
- Since each thread is working on one element of output `P`, we find `Row` and `Col` respectively (to compute `P[Row, Col]`)
    - Note that the algorithm assumes each block computes exactly one tile, so `TILE_WIDTH = BLOCK_SIZE`
    - Therefore `Row = by * TILE_WIDTH + ty`
    - And `Col = bx * TILE_WIDTH + tx`

Now we tackle the for loop part which loops through the phase counter `ph`.
- Recall that each phase uses one tile of `M` and one tile of `N`.
- The phase counter `ph` tells us how many phases we have already processed

### Loading Data into Shared Memory

For each phase, we first load data into shared memory:
- Each thread loads one value of `M` nad one value of `N` into shared memory
- Since we have `TILE_WIDTH x TILE_WIDTH` number of threads per block, in each phase we load this many elements into `Mds` and `Nds` respectively

Which data should each thread load?
- For `M`:
    - Note that `Row` already gives us the row of `M` to be loaded for this thread (constant across phases)
    - The beginning column for this block and phase is `ph * TILE_WIDTH`, because we have processed `ph` number of tiles to the left
    - So we want `M[Row, ph*TILE_WIDTH]`, which is indexed as `M[Row*Width + ph*TILE_WIDTH + tx]` in row major form
- For `N`:
    - `Col` already gives us the column of `N` to be loaded for this thread (constant across phases)
    - The beginning row for this block and phase is `ph * TILE_WIDTH`
    - So we want `N[ph*TILE_WIDTH + ty, Col]`, which is indexed as `N[(ph*TILE_WIDTH + ty)*Width + Col]`

Finally, we call `__syncthreads()` barrier synchronization. This is necessary because for the computation phase, each thread requires values of `Mds,Nds` that other threads have loaded for it. Hence we must make sure all threads have loaded into shared memory before we proceed to compute.
- This is called <<read-after-write>> synchronization - we require data to be written before we can read from it
- Also called <<true dependence>> because there is truly a dependence from the reader on the written data

### Computation Phase

Now we have loaded the requisite data into `Mds` and `Nds`, we can accumulate into `Pvalue`. The tiled matmul is such that we accumulate into `Pvalue` as though we were just doing a simple dot product between `Mds` and `Nds`. This compute phase is very fast as we are reading from shared memory.

Finally, another `__syncthreads()` is called. This is to ensure that computation has completed before we start loading values into `Mds` and `Nds` in the next phase, overwriting their values.
- This is called <<write-after-read>> synchronization - threads must wait for data to be read by all threads that need it before overwriting it
- Another name is <<false dependence>>. The dependence is false because the writing thread actually does not need any data from the reading thread - the dependence occurs purely because they are sharing the same memory location.

### Summary

The savings from tiled matmul is significant. With `16 x 16` tiles, we reduce global memory access by a factor of `16`, which brings our `0.25 FLOP/B` to `4 FLOP/B`. There are further optimizations we can make later.

## Boundary Checks

In the previous section, we assume that `Width` is a multiple of `TILE_WIDTH`. This section extends it to handle arbitrary matrix sizes. As with before, we need to add boundary checks to avoid accessing non-existent values or even accessing wrong values that corrupt the final output.

The modified kernel code is shown (only the for loop part):
```c
float Pvalue = 0;
for (int ph = 0; ph < ceil(Width/(float) TILE_WIDTH); ++ph) {
    if ((Row < Width) && (ph*TILE_WIDTH+tx) < Width)
        Mds[ty][tx] = M[Row*Width + ph*TILE_WIDTH + tx];
    else Mds[ty][tx] = 0.0f;
    if ((ph*TILE_WIDTH+ty) < Width && Col < Width)
        Nds[ty][tx] = N[(ph*TILE_WIDTH+ty)*Width + Col];
    else Nds[ty][tx] = 0.0f;
    __syncthreads();

    for (int k = 0; k < TILE_WIDTH; ++k) {
        Pvalue += Mds[ty][k] * Nds[k][tx];
    }
    __syncthreads();
}
if (Row < Width) && (Col < Width)
    P[Row*Width + Col] = Pvalue;
```

For boundary checks on `M`:
- As we recall, the `y` and `x` element accessed are `Row` and `ph*TILE_WIDTH + tx` respectively
- Hence we just need to check if both of these are less than `Width`

For boundary checks on `N`:
- The `y` and `x` element accessed are `(ph*TILE_WIDTH+ty)` and `Col` respectively
- Hence we just need to check if both of these are less than `Width`

For accessing `M` and `N`, if we are accessing an out of bounds value, simply load `0.0` into `Mds` and `Nds` respectively. This will ensure that when accumulating into `Pvalue`, using an out of bound value will result in adding `0.0` and hence not affect the final output.

Finally when assigning values into `P`, we just check if `Row, Col` are within bounds.

## Impact of Memory Use on Occupancy

Earlier on we saw that register usage can reduce occupancy. Similarly, using too much shared memory can reduce the number of blocks assigned and hence reduce occupancy.

For example, an A100 can have up to `164KB` of shared memory per SM and support a maximum of `2048` threads per SM.
- This means that to use up all thread slots, a thread block cannot use more than an average of `164 KB / 2048 threads = 82B / thread`

For our tiled matmul example:
- Each block has `TILE_WIDTH * TILE_WIDTH` threads per block
- Each block uses `4B * TILE_WIDTH * TILE_WIDTH` shared memory for `Mds` and `Nds` respectively
- So it uses `8B` per thread, which is less than `82B` and we are fine

But consider a kernel that uses `32KB` of shared memory, and each block has `256` threads.
- Then each thread uses on average `32KB / 256 = 128B`. This exceeds the `82B/thread` limit.
- So we can only launch at most `164KB / 128B = 1.3k threads`, which is around `60%` occupancy

### Dynamic sizing of shared memory

Since the size of shared memory on an SM is device specific, we want the host code to dynamically adjust the `TILE_WIDTH` and hence amount of shared memory used by a kernel.
- This can be done using `cudaGetDeviceProperties`, and using `devProp.sharedMemPerBlock` to give the amount of shared memory available in each SM.

Unfortunately, we cannot specify `Mds` and `Nds` as before using `TILE_WIDTH`, because the value of `TILE_WIDTH` must be available at compile time. If we want to change this value, we must recompile the code.

The workaround for such a scenario is to use `extern __shared__ Mds_Nds[];` inside the kernel code. This allows for a dynamically allocated shared array.
- However, since we only have one merged array, we'll need to manually define where `Mds` ends and `Nds` starts
- We also need to linearize our access to the array

At runtime, we can dynamically configure the amount of shared memory for each block using the **third** configuration parameter for a kernel call:

```c
size_t size = calculate_appropriate_SM_usage(devProp.sharedMemPerBlock, ...);

matrixMulKernel<<<dimGrid,dimBlock,size>>>(Md, Nd, Pd, Width, size/2, size/2);
```

Notes:
- Note that `size_t` is a built-in type for declaring a variable to hold the size information for dynamically allocated data structures, in number of bytes.
- Also note that we pass `size/2` for the size of `Mds` and `Nds` respectively

The kernel code can then be modified to accommodate the new structure:

```c
#define TILE_WIDTH 16
__global__ void matrixMulKernel(float* M, float* N, float* P, int Width,
                                    unsigned Mdz_sz, unsigned Nds_sz) {
    extern __shared__ char float Mds_Nds[];
    float *Mds = (float*) Mds_Nds;
    float *Nds = Mds_Nds + Nds_sz;
}
```

Note that we would need to use linearized accesses to `Mds` and `Nds`. e.g. `Mds[ty][tx]` becomes `Mds[ty*TILE_WIDTH+tx]`.

## Exercises