# Li 2021 - TaoBao Embedding-Based Retrieval

[Li 2021 - Embedding Based Product Retrieval in TaoBao Search](https://arxiv.org/abs/2106.09297)

This paper explains TaoBao's embedding-based search retrieval system. 

## Context

TaoBao's product search has many stages. This paper focuses on the initial retrieval stage, when around `10k` products are retrieved per query. The retrievers used are:
- Traditional lexical based term match retriever
- Item-based collaborative filtering retriever (presumably based on recent items)
- Embedding based retriever

The retrieval results from each retriever are merged and de-duplicated, and then sent to participate in the next phase (pre-ranking). This paper is only focused on the embedding-based retriever.

## Problem

Taobao tackles several problems in this paper:
- <<Poor control of relevance>> for embedding based retrieval (EBR). Taobao reported that over time, the EBR contributed to an increase in complaints as non-relevant items started to get surfaced. For example, if a query is made for `Nike Shoes`, shoes from similar brands like `Adidas` may get surfaced.
- <<Balancing relevance and personalization>>. Taobao notes that personalization is an important aspect of product search, and they propose a way of merging search relevance and personalization in this paper.

## Setup

Let $U = \{u_1, ..., u_N\}$ denote a set of users, $Q = \{ q_1, ..., q_N \}$ denote their corresponding queries, and $I = \{ i_1, ..., i_M \}$ denote the collection of items. Let us divide the user $u$'s historical sequence of items into 3 buckets:
- Real time, i.e. most recent $T$ item interactions, denote as $R^u = \{ i_1^u, ..., i_T^u \}$
- Short term, i.e. within last `10 days` but older than real time, denote as $S^u = \{ i_1^u, ..., i_T^u \}$
- Long term, i.e. within last `1 month` but older than short term, denote as $L^u = \{ i_1^u, ..., i_T^u \}$

The search task is as follows. We are given the historical behaviours $(R^u, S^u, L^u)$ of a given user and the submitted query $q_u$ at time $t$. We also get access to a sequence of historical queries for the user $q_{his} = \{ q_1^u, ..., q_k^u \}$. Our task is to return a set of items $i \in I$ that satisfy the search request. Typically, we score each `query, item` pair according to score $z$ and return the top-K items:
$$
    z = \mathcal{F}(\phi(q_u, R^u, S^u, L^u), \ \psi(i))
$$

Where:
- $\mathcal{F}$ denotes the scoring function, typically the inner product function
- $\phi$ denotes the query + behaviour encoder
- $\psi$ denotes the item encoder

## Model Architecture

The model architecture in the paper is quite complicated, although understandable after some time. I don't think it is profitable to describe the architecture in full specificity, but there are useful ideas within.

At a high level, the model architecture is a standard two-tower setup, where the user query and behaviours are encoded into a user embedding, and the item is encoded into an item embedding. The dot product is used to score user-item pairs and ANN search is used for retrieval. The interesting details lie mostly within the user tower, in particular the way the query and behaviours are integrated.

### Query Representation

We first tackle the query representation. Given a query string, the query encoder encodes it into $Q_{mgs} \in \R^{6 \times d}$, where $d$ is an arbitrary embedding dimension. The first dimension is `6` because the paper uses `6` different ways to encode the query (they call this multi-grained representation). The `6` representations are briefly explained as follows:
- <<1-gram>> and <<2-gram>> embeddings. The query string is tokenized into 1-gram or 2-gram and embeddings are looked up for each token. Mean pooling over tokens is done to get a single embedding.
- <<Phrase>> embeddings. Similarly, the query string is tokenized into phrases using their query segmentation engine. For example, 红色 is a phrase and 连衣裙 is a phrase. The same embedding lookup and mean-pooling is done. Call this $q_{seg}$ embedding.
- <<Phrase transformer>> embeddings. The phrase embeddings are also passed through a transformer before mean-pooling. I suppose the sequence of phrases matters enough to do this.
- <<Historical query>> embeddings. This is the interesting part where the query interacts with historical queries. 
    - Specifically, the phrase embedding $q_{seg} \in \R^d$ and the historical query matrix $q_{his} \in \R^{k \times d}$ is used to form an attention matrix in the form of $\text{attn} = \text{softmax}(q_{seg} \cdot q_{his}^T) \in \R^{1 \times k}$, which provides the relevance of each historical query to the current submitted query. 
    - The attention weights are then used to do a weighted average over the historical query embeddings $q_{his\_seq} = \text{attn} \cdot q_{his} \in \R^d$.
    - Hence we get a weighted representation of historical queries where more relevant queries to the current query are emphasized.
- <<Mix>> embeddings. This is simply a sum over the above `5` embeddings

### User Behaviour Representation

Now we tackle the representation of user behaviours. There are some minor deviations on how they treat $R^u, S^u, L^u$ respectively, but broadly they have 

