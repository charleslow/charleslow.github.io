# Lee 2022 - RQ-VAE

[Autoregressive Image Generation using Residual Quantization](https://arxiv.org/abs/2203.01941)

Vector quantization [VQ-VAE](./van_den_oord_2017.md) is used to represent an image as a sequence of discrete codes. After quantizing, an autoregressive model (AR model, e.g. transformer) is used to predict the codes.

This paper aims to reduce the sequence length of the discrete codes, as it becomes more computationally efficient for the AR model. However, reducing the sequence length (aka reducing the <<spatial resolution>>) causes a rate-distortion trade-off. The dilemma is such:
- Increasing the codebook size improves resolution of discretization to preserve quality
- But increasing the codebook size also increases the probability of codebook collapse, where only a subset of codes are used

The main idea of RQ-VAE is to reduce the spatial resolution by more precisely approximating the feature map at each location. Instead of increasing the codebook size, we use the codebook to recursively quantize the feature map in a coarse-to-fine manner. That is, the feature representation at each location is the sum of the selected codebook vectors at each level. For a codebook where each level has $K$ codebook vectors and $D$ levels, we can represent $K^D$ vectors for learning just $D \times K$ vectors. 