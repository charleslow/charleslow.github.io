# Lee 2020 - Large Scale Video Representation Learning

[Large Scale Video Representation Learning via Relational Graph Clustering](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Large_Scale_Video_Representation_Learning_via_Relational_Graph_Clustering_CVPR_2020_paper.pdf)

This paper proposes an efficient method to learn video representations from relational graph data (i.e. user item interactions). Specifically, it learns a small-ish embedding model that transforms raw audio-visual features for a given video into a vector representation that is useful for downstream recommendation tasks. This method seems to be in use at YouTube at least until 2023 as it was mentioned in [Singh 2023 - Better Generalization with Semantic IDs](./singh_2023.md).

This paper is in the genre of metric learning of similarity metric between videos. Similar to [FaceNet](./schroff_2015.md), it uses triplet contrastive loss to push related videos together and random videos apart. The contribution of the paper is a hierarchical clustering approach to sample <<smart negatives>> which they show to be much more informative for learning than random negatives. 

## Setup

We start with a raw input representation of a given video $x \in \R^d$, where $x$ could be a concatenation of various raw input features or a representation from some off-the-shelf pretrained embedding model. We are also given a relational graph $\mathcal{G} = (V, E)$ where each node is a video and each edge $(x_1, x_2) \in E$ represents some relationship between two videos. The edge weight can be binary or real numbers. We may obtain such edge relationships from implicit feedback, e.g. how frequently two videos are co-watched, co-clicked, co-searched, etc. The aim is to learn a representation $z \in \R^k$ where $k$ is much smaller than $d$, such that $z_1^T z_2 \sim 1$ if they are related and $0$ otherwise.  

## Graph Clustering

The relational graph is pre-processed with a hierarchical clustering algorithm. A hierarchical algorithm is chosen so that we can sample negatives with varying levels of difficulty later on. The paper uses <<Affinity Clustering>> from [Bateni 2017 - Affinity Clustering](https://papers.nips.cc/paper_files/paper/2017/file/2e1b24a664f5e9c18f407b2f9c73e821-Paper.pdf), although it notes that any suitable clustering algorithm works as well. The naive algorithm for affinity clustering (closely following Boruvka's algorithm) is as follows:
- Start with every node in its own cluster
- In each round, find the highest weighted outgoing edge from each cluster $c_i$ and add the edge
    - i.e. for each cluster $c_i$, find $\argmax_{u, v} \ \{ weight(u, v): u \in c_i, v \notin c_i  \}$
    - Note that this step ensures that the number of clusters at least halves, since each cluster is connected to another cluster
- If the number of clusters becomes lower than $k$, we undo the most recent edge added until we get $k$ clusters
- At the end of each round, we have the desired $k$ number of clusters
- The next round then commences with these obtained clusters

Importantly, the algorithm is able to proceed in a distributed manner because in each round, we only need access to the subset of edges with a node in cluster $c_i$ in order to find the outgoing edge from cluster $c_i$. Will defer understanding the distributed algorithm to a later time.

A desirable property of affinity clustering (as with Boruvska's algorithm) is that it tends to produce clusters of similar size. This can be intuitively seen since at each step, every cluster gets linked to at least one other cluster, hence even the smallest cluster will have its size doubled. This is in contrast to Kruskal's algorithm which finds a global edge at each turn, hence tends to result in more unequal clusters. Bateni 2017 ran some experiments demonstrating that this indeed the case when running these algorithms on real world datasets. Having clusters of similar size is helpful for the sampling approach in this paper because the number of negatives in each cluster would be similar.




