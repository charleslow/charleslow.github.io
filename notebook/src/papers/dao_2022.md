# Dao 2022 - Flash Attention

[Paper Link](https://arxiv.org/pdf/2205.14135.pdf)

This paper argues that the attention mechanism is slow because of reading / writing between GPU High Bandwidth Memory and GPU on-chip SRAM. The authors hence create a block-wise attention algorithm that minimizes such IO read / writes and speeds up attention significantly especially when the sequence length is long.

## Brief Overview of Attention

Suppose we have an input sequence of embeddings $X = (x_1, ..., x_N)$ where $x_i \in \mathbb{R}^k$, such that $X \in \mathbb{R}^{k \times N}$. Naively, we can compute activations by $V = X^T \cdot W_v$, where $W_v \in \mathbb{R}^{k \times d}$, such that $V \in \mathbb{R}^{N \times d}$. However, this naive way of encoding our input sequence does not allow interaction between inputs at different positions (say $x_i$ with $x_j$). We can see this by observing that the first row of $V$ is only affected by the first column of $X$ (i.e. first encoding $x_1$), and likewise for all the other positions.

Attention addresses this problem by adding an interaction mechanism. Besides $W_v$, we also create weight parameters $W_q, W_k \in \mathbb{R}^{k \times d}$. Given an input $X$, we compute $Q, K, V \in \mathbb{R}^{N \times d}$ as follows:
- $Q = X^T \cdot W_q$
- $K = X^T \cdot W_k$
- $V = X^T \cdot W_v$

We then create an interaction matrix $S = QK^T \in \mathbb{R}^{N \times N}$, and apply row-wise softmax to get $P = softmax(S) \in \mathbb{R}^{N \times N}$. $S$ can be thought of as a pairwise similarity matrix between the encoding at position $i$ and position $j$ that captures the degree of interaction. For example, in a sentence `the economy has been in decline`, the value of $S_{1,5}$ (assuming 0-index) measuring the interaction between `economy` and `decline` might be high.

Finally, we produce the output $O = PV \in \mathbb{R}^{N \times d}$, which is an activation output from the input sequence that has captured the interactions between tokens at different positions of the input. This simple mechanism has led to significant improvements in language modelling.

## GPU Memory Hierarchy

The memory hierarchy is such that read/write speed is super fast on the SRAM but memory is highly limited. Hence, the N x N attention matrix is written/read repeatedly to/from HBM, resulting in IO being a bottleneck. The numbers are as such on an A100 GPU:
- SRAM: 19 TB/s (20 MB RAM)
- HBM: 1.5 TB/s (40 GB RAM)

## Naive Attention Algorithm

The naive attention algorithm has many reads and writes to HBM. (ps: Not sure why we cannot persist the intermediate matrices on SRAM and complete the computations, but in any case the naive algorithm requires materializing the $N \times N$ matrices on SRAM which will quickly flood it. For example, a sequence length of `2,048` at `float32` already takes up `33MB` for the $S$ matrix).

1. Load $Q, K$ from HBM, compute $S = QK^T$, write $S$ to HBM
2. Read $S$ from HBM, compute $P = softmax(S)$, write $P$ to HBM
3. Load $P, V$ from HBM, compute $O = PV$, write $O$ to HBM

## Flash Attention

The main idea is quite simple: instead of computing the full attention matrix, we use block-wise tiling to compute parts of it at a time. This reduces the memory required for each block and allows the whole computation to be done on SRAM while minimizing the amount of IO read from HBM, leading to faster compute time and lower memory usage on SRAM. The difficulty is in devising a block-wise softmax algorithm that yields the exact same result as computing it all at once.

Consider the naive softmax algorithm on an arbitrary vector $x \in \mathbb{R}^B$.

$$
\begin{aligned}
m(x) &:= \max_{i} \ x_i
\\
f(x) &:= [e^{x_i - m(x)} \quad ... \quad e^{x_B - m(x)}]
\\
l(x) &:= \sum_i f(x)_i
\\
softmax(x) &:= \frac{f(x)}{l(x)}
\end{aligned}
$$

Note that the maximum value $m(x)$ is subtracted for numerical stability to avoid overflow (underflow is ok because $e^{-inf} = 0$). $f(x) \in \mathbb{R}^B$ is the numerator and $l(x) \in \mathbb{R}$ is the sum of all elements in $f(x)$.

Now, the problem with the naive softmax algorithm in the context of attention is that we need an entire row of $S$ ($N$ elements) to perform the row-wise softmax computation. This will not be available if we are performing block-wise computation, since we are splitting $Q, K \in \mathbb{R}^{Nxd}$ row-wise into blocks of $Q_i, K_j \in \mathbb{R}^{Bxd}$. When we compute $S_{ij} := Q_i \cdot K_j^T \in \mathbb{R}^{BxB}$, blocks of $S$ will be materialized in each pass, but not the entire row at a time.

Hence, we need a modified algorithm that allows us to compute chunks of the final output $O \in \mathbb{R}^{Nxd}$ at a time by iterating block-wise through $S$, such that the combination of the new chunk of $O$ at each step with the already written intermediate $O$ gives the correct result at the end. The key to realizing this algorithm is in decomposing the softmax step, as shown below.

Consider two vectors $x^a, x^b \in \mathbb{R}^B$. We can decompose the softmax of their concatenated vector $x = [x^a \ x^b] \in \mathbb{R}^{2B}$ as follows:

$$
\begin{aligned}
m(x) &= max(m(x^a), m(x^b))
\\
f(x) &= [e^{m(x^a) - m(x)} \cdot f(x^a) \quad e^{m(x^b) - m(x)} \cdot f(x^b)]
\end{aligned}
$$

The first line of the above simply notes that the maximum of $x$ is the maximum over each of the maximums of the subvectors $x^a, x^b$. The second line also simply notes that we previously modified each element of $f(x)$ by a factor, say $e^{-m(x^a)} \cdot e^{x_i}$ for those in $x_a$ (the modifier being $e^{-m(x^a)}$). To get the correct modifier, we need to divide by the 
