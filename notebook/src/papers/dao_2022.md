# Dao 2022 - Flash Attention

[Paper Link](https://arxiv.org/pdf/2205.14135.pdf)

This paper argues that the attention mechanism is slow because of reading / writing between GPU High Bandwidth Memory and GPU on-chip SRAM. The authors hence create a block-wise attention algorithm that minimizes such IO read / writes and speeds up attention significantly especially when the sequence length is long.

## Brief Overview of Attention

Suppose we have an input sequence of embeddings $X = (x_1, ..., x_N)$ where $x_i \in \mathbb{R}^k$, such that $X \in \mathbb{R}^{k \times N}$. Naively, we can compute activations by $V = X^T \cdot W_v$, where $W_v \in \mathbb{R}^{k \times d}$, such that $V \in \mathbb{R}^{N \times d}$. However, this naive way of encoding our input sequence does not allow interaction between inputs at different positions (say $x_i$ with $x_j$). We can see this by observing that the first row of $V$ is only affected by the first column of $X$ (i.e. first encoding $x_1$), and likewise for all the other positions.

Attention addresses this problem by adding an interaction mechanism. Besides $W_v$, we also create weight parameters $W_q, W_k \in \mathbb{R}^{k \times d}$. Given an input $X$, we compute $Q, K, V \in \mathbb{R}^{N \times d}$ as follows:
- $Q = X^T \cdot W_q$
- $K = X^T \cdot W_k$
- $V = X^T \cdot W_v$

We then create an interaction matrix $S = QK^T \in \mathbb{R}^{N \times N}$, and apply row-wise softmax to get $P = softmax(S) \in \mathbb{R}^{N \times N}$. $S$ can be thought of as a pairwise similarity matrix between the encoding at position $i$ and position $j$ that captures the degree of interaction. For example, in a sentence `the economy has been in decline`, the value of $S_{1,5}$ (assuming 0-index) measuring the interaction between `economy` and `decline` might be high.

Finally, we produce the output $O = PV \in \mathbb{R}^{N \times d}$, which is an activation output from the input sequence that has captured the interactions between tokens at different positions of the input. This simple mechanism has led to significant improvements in language modelling.

## GPU Memory Hierarchy

The memory hierarchy is such that read/write speed is super fast on the SRAM but memory is highly limited. Hence, the N x N attention matrix is written/read repeatedly to/from HBM, resulting in IO being a bottleneck. The numbers are as such on an A100 GPU:
- SRAM: 19 TB/s (20 MB RAM)
- HBM: 1.5 TB/s (40 GB RAM)

## Naive Attention Algorithm

The naive attention algorithm has many reads and writes to HBM. (ps: Not sure why we cannot persist the intermediate matrices on SRAM and complete the computations, but in any case the naive algorithm requires materializing the $N \times N$ matrices on SRAM which will quickly flood it. For example, a sequence length of `2,048` at `float32` already takes up `33MB` for the $S$ matrix).

1. Load $Q, K$ from HBM, compute $S = QK^T$, write $S$ to HBM
2. Read $S$ from HBM, compute $P = softmax(S)$, write $P$ to HBM
3. Load $P, V$ from HBM, compute $O = PV$, write $O$ to HBM

## Flash Attention

The main idea is quite simple: instead of computing the full attention matrix, we use block-wise tiling to compute parts of it at a time. This reduces the memory required for each block and allows the whole computation to be done on SRAM while minimizing the amount of IO read from HBM, leading to faster compute time and lower memory usage on SRAM. The slight difficulty is in devising a block-wise softmax algorithm that yields the same result.

We load each $O_i$ (subset of rows) multiple times as we iterate through $K_j, V_j$. Each time, we update $O_i$ using the softmax algo.



