# Probability, Entropy, Inference

This chapter will spend some time on notation.

## Probabilities and ensembles

> An <<ensemble>> $X$ is a triple $(x, \A_X, \P_X)$ where the outcome $x$ is the value of a random variable, which takes on one of a set of possible values from $\A_X = \{ a_1, ..., a_I \}$. $\P_X = \{ p_1, ..., P_I \}$ represents the probability distribution, such that $P(x = a_i) = p_i$, $p_i \geq 0$, and $\sum_{a_i} P(x=a_i) = 1$

The name $\A$ is a mnemonic for "alphabet". An example of an ensemble is to select a random latter from an english document.

Note: Mackay's definition of an ensemble seems redundant, as we usually just use the term "random variable" to denote the same idea. But he insists on using "ensemble" to refer to the entire system or environment, with an emphasis on the total space of the alphabet. Whereas in strict mathetical form, a random variable refers to a function that maps from each outcome to a real number.

> **Probability of a subset**. If $T$ is a subset of $\A_X$, then:
$$
    P(T) = P(x \in T) = \sum_{a_i \in T} P(x=a_i)
$$

> **Joint ensemble** $XY$ is an ensemble in which each outcome is an ordered pair $x, y$ with $x \in \A_X = \{ a_1, ..., a_I \}$ and $y \in \A_Y = \{b_1, ..., b_J \}$.

We call $P(x, y)$ or $P(xy)$ the joint probability of $x$ and $y$.

> **Marginal probability**. We can obtain the marginal probability $P(x)$ from the joint probability $P(x, y)$ by summation:
$$
    P(x = a_i) \equiv \sum_{y \in \A_Y} P(x=a_i, y)
$$

Or more succintly:
$$
    P(y) \equiv \sum_{x \in \A_X} P(x, y)
$$

> **Conditional Probability**. 
$$
    P(x = a_i | y = b_j) \equiv \frac{P(x=a_i, y=b_j)}{P(y=b_j)} \ \ \ \text{ if } P(y=b_j) \neq 0
$$

We often do not write down the joint probability directly, but rather define an ensemble in terms of a collection of conditional probabilities. Hence the following rules to manipulate conditional probabilities are useful.

> **Product rule (or Chain rule)**. Based on the definition of conditional probability, we have:
$$
    P(x, y | \H) = P(x | y, \H) P(y | \H) = P(y | x, \H) P(x | \H)

$$

Where $\H$ is a way of denoting the assumptions upon which the probabilities are based.

> **Sum Rule**. We are just writing the marginal probability in terms of conditional probabilities:
$$
\begin{align*}
    P(x | \H) &= \sum_y P(x, y | \H) \\
    &= \sum_y P(x | y, \H) P(y | \H)
\end{align*}
$$

> **Bayes theorem** - obtained from the product rule:
$$
\begin{align*}
    P(y | x, \H) &= \frac{P(x | y, \H)P(y | \H)}{
        P(x | \H)
    } \\
    &= \frac{
        P(x | y, \H) P(y | \H)
    }{
        \sum_{y'} P(x | y', \H) P(y' | \H)
    }
\end{align*}
$$

> **Independence** Two random variables $X$ and $Y$ are independent if and only if:
$$
    P(x, y) = P(x)P(y)
$$

We often define an ensemble in terms of a collection of conditional probabilities. This example illustrates the idea.

> **Example 2.3**. Jo has a test for a disease. The variable $a$ denotes whether Jo has the disease and $b$ denotes the test result. In $95%$ of cases of people who have the disease, a positive test results; in $95%$ of cases of people who do not have the disease, a negative test results. $1%$ of people of Jo's age and background have the disease. Now Jo takes the test and it is positive. What is the probability that Jo has the disease?

Based on the information:
$$
\begin{align*}
    P(B=0 | A=0) &= 0.95\\
    P(B=1 | A=1) &= 0.95\\
    P(A=1) &= 0.01\\
\end{align*}
$$

We want:
$$
\begin{align*}
    P(A=1 | B=1) &= \frac{
        P(B=1 | A=1) P(A=1)
    }
    {
        P(B=1)
    }\\
    &= \frac{
        P(B=1 | A=1) P(A=1)
    }{
        P(B=1 | A=0)P(A=0) + P(B=1 | A=1)P(A=1)
    }\\
    &= \frac{
        0.95 \times 0.01
    }{
        0.05 \times 0.99 + 0.95 \times 0.01
    }\\
    &= 0.16
\end{align*}
$$

## The meaning of probability

There are two philosophical definitions of probability.

The <<frequentist>> view is easier to grasp and clearly defined in the case of random variables. It is simply the frequency of outcomes in random experiments. This is well defined in the case of random variables where the concept of repeated trials is valid. However, it does not translate neatly into real world scenarios - what does it mean for the probability that x murdered y given the amount of evidence available?

The <<bayesian>> view tries to account for such by using the concept of degrees of belief. This notion of degrees of belief can be mapped to probabilities is they satisfy simple consistency rules known as the Cox axioms. 

## Forward probabilities and inverse probabilities

There are two categories of probability calculations: <<forward>> and <<inverse>>.

> **Example 2.4**. This is an example of a forward probability problem. An urn contains $K$ balls, of which $B$ are black and the rest are white. Fred draws a ball at random from the urn and replaces it, $N$ times.
> 
> a. What is the probability distribution of the number of times a black ball is drawn, $n_B$?
> 
> b. What is the expectations and variance of $n_B$?

We note that $n_B$ follows a binomial distribution:
$$
    n_B \sim \text{Bin}(N, \frac{B}{K})
$$

The expectation is $NB/K$ and the variance is $N(B/K)(1-B/K)$.

The following is an <<inverse probability>> problem. Instead of computing the probability distribution of some quanity produced by the process, we compute the conditional probability of one or more of the unobserved variables in the process. This invariably requires use of bayes' theorem.

> **Example 2.6**. There are eleven urns labelled by $u \in \{0 ,..., 10 \}$. Each urn contains $10$ balls. For urn $u$, $u$ balls are black and the rest are white. Fred selects an urn at random and draws $N$ times with replacement, obtaining $n_B$ black balls and $N-n_B$ white balls. If after $N=10$ draws $n_B=3$ blacks have been drawn, what is the probability that the urn Fred is drawing from is any urn $u$?

We want to find $P(u | n_B, N)$:
$$
\begin{align*}
    P(u | n_B, N) &= \frac{P(u, n_B | N)}{P(n_B | N)}\\
    &= \frac{P(n_B | u, N) P(u)}{P(n_B | N)}
\end{align*}
$$

$P(u)$ is straightforward as it is $\frac{1}{11}$ for all $u$ by definition. 

For $P(n_B | u, N)$:
$$
\begin{align*}
    P(n_B | u, N) = \binom{N}{n_B} f_u^{n_B} (1-f_u)^{N-n_B}
\end{align*}
$$

Where $f_u$ is the probability of choosing a black ball in urn $u$, i.e. $u/10$.

The final term is the denominator, $P(n_B | N)$. This is simply the sum of what we wrote above for $P(n_B | u, N)$ over all possible instances of $u$. i.e.
$$
    P(n_B | N) = \sum_u P(u) P(n_B | u, N)
$$

With these formulas, the rest is just arithmetic. For the settings in the question, the numbers are like:
- $P(u=0 | n_B=3, N=10) = 0$
- $P(u=1 | n_B=3, N=10) = 0.063$
- $P(u=2 | n_B=3, N=10) = 0.22$
- $P(u=3 | n_B=3, N=10) = 0.29$

In inverse probability problems it is useful to give names to the different terms in bayes theorem:
- The probability $P(u)$ is called the <<prior probability>> of $u$
- $P(n_B | u, N)$ is called the <<likelihood of $u$>>. 
    - It is important to note that $P(n_B | u, N)$ is sometimes called the probability of $n_B$ given $u$, if we fix $u$ and want to express the probability of the observed data
    - But if we fix $n_B$ (the data) and want to express the likelihood of the parameters, then $P(n_B | u, N)$ is called the likelihood of $u$
- $P(u | n_B, N)$ is called the <<posterior probability of $u$>> given $n_B$. 
- $P(n_B | N)$ is known as the evidence or <<marginal likelihood>> 

In summary, let $\theta$ denote the unknown parameters, $D$ the data, and $\H$ the hypothesis space. Then we have:
$$
    P(\theta | D, \H) = \frac{P(D | \theta, \H) P(\theta | \H)}{P(D | \H)}
$$

This is also known as:
$$
    \text{posterior} = \frac{\text{likelihood} \times \text{prior}}{\text{evidence}}
$$

> **Example 2.6 continued.** Assume again that we observed $n_B = 3, N=10$. Let us draw another ball from the same urn. What is the probability that the next draw results in a black ball?

We should use our expression of the posterior probabilities we calculated earlier. Note that we do not have a fixed guess of which particular urn $u$ it is - we only have a probability distribution over all urns (with the highest probability placed on urn $u=3$).

Hence, we need to marginalize over all possible urns. The probability that the next ball is black (given that we fix urn $u$) is simply $f_u$. So we want:
$$
    P(\text{next ball is black} | n_B, N) = \sum_u f_u P(u | n_B, N)
$$

Doing all the calculations will give us $0.333$ as the answer.

Note that this differs from the answer if we fixed our guess at the maximum likelihood solution, which is $u=3$. This would yield $0.3$ as the answer. Marginalizing over all possible values of $u$ yields a more robust answer.

## Data compression and inverse probability

Suppose we have a binary file that is just random sequence of $0$ and $1$s, something like:
```
000000000000000011111111111111111000000000000000001111111110101011101111111111
```

Intuitively, compression works by taking advantage of the predictability of a file. A data compression program must, implicitly or explicitly, answer the question "What is the probability that the next character in this file is a `1`?".

Mackay's take is that data compression and data modelling are one and the same, and this is basically an inverse probability problem. More of this in chapter `6`.

## Likelihood principle

The likelihood principle tells us that the only thing that matters for inference problems is the likelihood, i.e. how the probability of the data that was observed varies with the hypothesis. In other words:

> The <<likelihood principle>>. Given a generative model for data $d$ given parameters $\theta, P(d | \theta)$ and having observed a particular outcome $d_1$, all inferences and predictions should depend only on the function $P(d_1 | \theta)$.

## Definition of entropy

> **Shannon information content** of an outcome $x$ is defined as:
$$
    h(x) = \log_2 \frac{1}{P(x)}
$$

The unit of measurement is *bits*. In the subsequent chapters, it will be established that the information content $h(a_i)$ is indeed a natural measure of the information content of the event $x=a_i$. 

> **Entropy** of an ensemble $X$ is defined as the average shannon information content of each outcome:
$$
    H(X) \equiv \sum_{x \in \A_X} P(x) \log \frac{1}{P(x)}
$$

The convention for $P(x) = 0$ is $h(x) = 0$, since $\lim_{\theta \rightarrow 0^+} \theta \log 1 / \theta = 0$.

Where it is convenient, we may also write $H(X)$ as $H(\mathbf{p})$, where $\mathbf{p}$ is the vector of probability $p_1, p_2, ..., p_I$. 

> **Example 2.12**. The entropy of a randomly selected letter in an English document is about $4.11$ bits.

## Properties of Entropy

Here we cover some properties of the entropy function.
- $H(X) \geq 0$, with equality iff $p_i = 1$ for one single event $i$
- Entropy is maximized if $\mathbf{p}$ is uniform:
$$
    H(X) \leq \log(|\A_X|)
$$
    - With equality iff $p_i = 1 / |\A_X|$ for all $i$
    - Note that $|\A_X|$ is the number of elements in $\A_X$

> **Redundancy**. The redundancy of $X$ is:
$$
    1 - \frac{H(X)}{\log | \A_X |}   
$$

Intuitively, the redundancy measures the difference between $H(X)$ and its maximum possible value which is $\log( | \A_X | )$. 

> **Joint Entropy**. The joint entropy of $X, Y$ is:
$$
    H(X, Y) = \sum_{x,y \in \A_X, \A_Y} P(x, y) \log \frac{
        1
    }{
        P(x, y)
    }
$$

Entropy is additive for independent random variables:
$$
    H(X, Y) = H(X) + H(Y) \ \ \ \text{ iff } P(x, y) = P(x)P(y)
$$

## Decomposability of Entropy

One useful property of the Shannon entropy is that it can be decomposed into two parts, if we group outcomes together:
- The entropy of grouped outcomes
- Plus the weighted entropy of the outcomes within each group

Let $X$ be a discrete random variable with $n$ distinct outcomes $x_1, x_2, .., x_n$ and associated probabilities $p_1, p_2, ..., p_n$. 

We partition these outcomes into $m$ disjoint groups $S_1, S_2, ..., S_m$. Let $q_k$ be the probability of group $S_k$:
$$
    q_k = \sum_{j \in S_k} p_j
$$

> The decomposability theorem then states that:
$$
    H(X) = H(Q) + \sum_{k=1}^m q_k H_k
$$

Where $H_k$ is the entropy within each group. 

**Proof**. Let us first define the conditional probability of a specific outcomes $x_j$, given that we are already inside group $S_k$, as $r_{j|k}$:
$$
    r_{j|k} = \frac{p_j}{q_k}
$$

Now we decompose the entropy of the full system:
$$
\begin{align*}
    H(X) &= -\sum_{j=1}^n p_j \log p_j \\
    &= - \sum_{k=1}^m \sum_{j \in S_k} p_j \log p_j \\
    &= - \sum_{k=1}^m \sum_{j \in S_k} (q_k r_{j|k}) \log (q_k r_{j|k}) \\
    &= - \sum_{k=1}^m \sum_{j \in S_k} (q_k r_{j|k}) \log q_k
        - \sum_{k=1}^m \sum_{j \in S_k} (q_k r_{j|k}) \log r_{j|k}
\end{align*}
$$

For the first term, it is just the entropy at the group level:
$$
\begin{align*}
    \text{First term } &= - \sum_{k=1}^m \sum_{j \in S_k} (q_k r_{j|k}) \log q_k\\
        &= - \sum_{k=1}^m q_k \log q_k \sum_{j \in S_k} r_{j|k}\\
        &= - \sum_{k=1}^m q_k \log q_k \\
        &= H(Q)
\end{align*}
$$

Note that in the second line, the summation over $r_{j|k}$ is equals to $1$ as we sum up the probabilities of all events in a group.

For the second term, it is the weighted entropies for each group:
$$
\begin{align*}
    \text{Second term } &= - \sum_{k=1}^m \sum_{j \in S_k} (q_k r_{j|k}) \log r_{j|k}\\
    &= - \sum_{k=1}^m q_k \sum_{j \in S_k} r_{j|k} \log r_{j|k}\\
    &= \sum_{k=1}^m q_k H_k
\end{align*}
$$

Note that in the second line, the inner sum is simply the entropy of outcomes within group $k$.

Hence we have shown the decomposability theorem.

--- 

> **Example 2.13**. A source produces a character $x$ from the alphabet $\A = \{ 0, 1, ..., 9, a, b, ..., z \}$. With equal probability, $x$ is first determined to be a numeral, vowel or consonant. Then, within the selected category, a random element is selected. What is the entropy of $X$?

Using the decomposability theorem:
$$
\begin{align*}
    H(X) &= H(Q) + \sum_{k=1}^3 \frac{1}{3} H_k\\
        &= \log 3 + \frac{1}{3} (\log 10 + \log 5 + \log 21)\\
        &\approx 4.9
\end{align*}
$$

## Gibb's Inequality

> **Relative Entropy**. The relative entropy of kullback-leibler divergence between two probability distributions $P(x)$ and $Q(x)$ that are defined over the same alphabet $\A_X$ is:
$$
    D_{\text{KL}}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

The relative entropy satisfies Gibb's inequality:
$$
    D_{\text{KL}}(P || Q) \geq 0
$$

With equality iff $P = Q$. 

