# Probability, Entropy, Inference

This chapter will spend some time on notation.

## Probabilities and ensembles

> An <<ensemble>> $X$ is a triple $(x, \A_X, \P_X)$ where the outcome $x$ is the value of a random variable, which takes on one of a set of possible values from $\A_X = \{ a_1, ..., a_I \}$. $\P_X = \{ p_1, ..., P_I \}$ represents the probability distribution, such that $P(x = a_i) = p_i$, $p_i \geq 0$, and $\sum_{a_i} P(x=a_i) = 1$

The name $\A$ is a mnemonic for "alphabet". An example of an ensemble is to select a random latter from an english document.

> **Probability of a subset**. If $T$ is a subset of $\A_X$, then:
$$
    P(T) = P(x \in T) = \sum_{a_i \in T} P(x=a_i)
$$

> **Joint ensemble** $XY$ is an ensemble in which each outcome is an ordered pair $x, y$ with $x \in \A_X = \{ a_1, ..., a_I \}$ and $y \in \A_Y = \{b_1, ..., b_J \}$.

We call $P(x, y)$ or $P(xy)$ the joint probability of $x$ and $y$.

> **Marginal probability**. We can obtain the marginal probability $P(x)$ from the joint probability $P(x, y)$ by summation:
$$
    P(x = a_i) \equiv \sum_{y \in \A_Y} P(x=a_i, y)
$$

Or more succintly:
$$
    P(y) \equiv \sum_{x \in \A_X} P(x, y)
$$

> **Conditional Probability**. 
$$
    P(x = a_i | y = b_j) \equiv \frac{P(x=a_i, y=b_j)}{P(y=b_j)} \ \ \ \text{ if } P(y=b_j) \neq 0
$$

We often do not write down the joint probability directly, but rather define an ensemble in terms of a collection of conditional probabilities. Hence the following rules to manipulate conditional probabilities are useful.

> **Product rule (or Chain rule)**. Based on the definition of conditional probability, we have:
$$
    P(x, y | \H) = P(x | y, \H) P(y | \H) = P(y | x, \H) P(x | \H)

$$

Where $\H$ is a way of denoting the assumptions upon which the probabilities are based.

> **Sum Rule**. We are just writing the marginal probability in terms of conditional probabilities:
$$
\begin{align*}
    P(x | \H) &= \sum_y P(x, y | \H) \\
    &= \sum_y P(x | y, \H) P(y | \H)
\end{align*}
$$

> **Bayes theorem** - obtained from the product rule:
$$
\begin{align*}
    P(y | x, \H) &= \frac{P(x | y, \H)P(y | \H)}{
        P(x | \H)
    } \\
    &= \frac{
        P(x | y, \H) P(y | \H)
    }{
        \sum_{y'} P(x | y', \H) P(y' | \H)
    }
\end{align*}
$$