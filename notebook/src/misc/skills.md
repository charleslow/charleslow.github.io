# Skills

This documents LinkedIn's approach to constructing a knowledge graph around skills.

## [November 2022 - Building LinkedIn's Skills Graph to Power a Skills-First World](https://www.linkedin.com/blog/engineering/skills-graph/building-linkedin-s-skills-graph-to-power-a-skills-first-world)

An important model for LinkedIn is the skills graph - it maps `39k skills` over `26 languages` and over `347k aliases`. The components of their structured skills graph:
- `39k` nodes, each of which is a skill
- Each skill has multiple aliases
- Skills are connected via edges which specify a hierarchical relationship. (e.g. `Marketing` is a `parent` of `Demand Generation` which is a parent of `Email Marketing`)

<<Skills Usage>>. Skills are automatically extracted from job postings and member histories. Job postings are allowed to tag up to 10 skills. Users can see something like `4/10 skills match your profile` to help them determine job fit.

The skills graph is constructed both using ML and manual review by taxonomists.

## [March 2023 - Building and maintaining the skills taxonomy that powers LinkedIn's Skills Graph](https://www.linkedin.com/blog/engineering/data/building-maintaining-the-skills-taxonomy-that-powers-linkedins-skills-graph)

This article is more about how the <<Skills Graph>> is constructed.

Each skill node includes the following foundational details (e.g. `Machine Learning`):
- Description of the skill: `the study of computer algorithms...`
- Aliases: `ML, ...`
- Skills type: `Hard` or `Soft`
- Skill ID

Each skill is represented by a `node` in the graph and nodes are linked via edges called `knowledge lineages`. Skills can relate for various reasons. Edges are directed and represent a parent-child relationship (e.g. `Software Development` -> `Back-end Software Development` -> `Python`)

Each node can have multiple parents and/or children. 
- Multiple parents example: Both `Back-end Software Development` and `Mobile Software Development` are parents of `Java`
- Multiple children example: `Supply Chain Management` has children `Supply Chain Engineering`, `Logistics Management` and `Digital Supply Chain`

The hierarchical relationship allows us to enrich skills understanding, since if a person knows a particular skill, we may infer that he knows something about all the parent nodes and perhaps some of the "sibling" nodes.

LinkedIn has certain quality guardrails on the structured skills, one of which is <<discouraging ambiguity>>. For example, an ambiguous skill like `Networking` may be mapped to skills like `Computer Networking` or `Professional Networking`. This type of ambigious relationship is not allowed, and LinkedIn either removes such edges or disallows such a node. The meaning of a phrase is determined by analyzing how the skill is used predominantly in LinkedIn. In cases where a phrase can have divergent meanings, the skill is disambiguated by expanding the phrase. For example, `Cadence` is disambiguated to `Cadence Software` and `Boundary` to `Boundary Line`.

<<Architecture>>. The components to their ecosystem are as follows:
1. <<Human Curated KG>>. Presumably, this is a purely human-curated KG where all nodes and edges are taken as true, and constantly curated by human taxonomists.
2. <<AI-generated KG>>. This is generated using ML models which use the human curated KG as training and validation data. The model behind this is <<KGBERT>>, which will be briefly covered below.
3. <<Serving>>. Both the AI-generated KG and human-generated KG are made available to all LinkedIn services via a REST API for online serving and also on HDFS to power offline inference.

<<KGBERT>> is a model for automatic edge prediction, which can generate the AI-generated KG from the human curated one. The basic idea is that the human curated KG is used to generate training and validation data in the following form:

```
[CLS] Tensorflow [SEP] Machine Learning [SEP] -> label: child - parent
```

Two skills are concatenated to form the context. The skill is represented at random by its `title`, `description` or `title + description`. The context is fed into a BERT model and a linear + softmax layer is attached to the `[CLS]` token to generate a softmax probability over 3 options:
- Parent -> Child
- Child -> Parent
- No relation

The <<positive labels>> are taken from edges in the human curated graph. The <<negative labels>> are generated by some heuristics:
- Skill pairs from different industries
- Niece / Nephew pairs e.g. `Tensorflow` vs `Cognitive Computing`
- Sibling pairs with the same parent e.g. `Tensorflow` vs `Pytorch`
- Loosely related pairs that are 3 or more steps apart

The nice thing about this architecture is the <<clean separation>> of the human curated data from the AI-generated one. The human-in-the-loop setup allows humans to review the AI-generated graph and add new edges to the human curated graph, which in turn improves the AI-generated graph. This seems superior over mixing both AI-generated edges and human-curated edges in the same graph.

